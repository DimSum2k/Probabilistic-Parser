{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint, pformat\n",
    "from utils import read, split, build_vocabulary, extract_sentences\n",
    "from PCFG import PCFG\n",
    "from OOV import OOV\n",
    "from Word2Vec import Word2Vec\n",
    "from Ngram import Automaton\n",
    "from main import main\n",
    "import nltk\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import progressbar\n",
    "\n",
    "from PYEVALB import scorer\n",
    "from PYEVALB import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train corpus length : 2790 (90%)\n",
      "Valid corpus length : 0 (0%)\n",
      "Test corpus length : 309 (Last 10%) \n",
      "\n",
      "Train vocabulary size:  9633\n",
      "100004 (100004, 64)\n",
      "2177/9633 words from train set not in polyglot embedding\n",
      "9633 (9633, 64)\n"
     ]
    }
   ],
   "source": [
    "[corpus_train, \n",
    " corpus_val, \n",
    " corpus_test, \n",
    " sentences_train, \n",
    " POS_train, \n",
    " sentences_val, \n",
    " POS_val, \n",
    " sentences_test, \n",
    " POS_test,\n",
    "vocabulary,\n",
    "w2v,\n",
    "ngram,\n",
    "oov] = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcfg = PCFG()\n",
    "pcfg.fit(corpus_train)\n",
    "#allPOS = list(set(pcfg.POS).union(pcfg.pcfg_.keys()))\n",
    "#print(len(allPOS))\n",
    "#allPOS.pop(allPOS.index(\"SENT\"))\n",
    "#print(len(allPOS))\n",
    "#allPOS = [\"SENT\"] + allPOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_CYK(sentence,pcfg):\n",
    "    \n",
    "    r = len(pcfg.non_terminals) \n",
    "    n = len(sentence)\n",
    "    P = np.zeros((n,n,r))\n",
    "    back = np.empty((n,n,r),dtype=np.ndarray)\n",
    "\n",
    "    for s,word in enumerate(sentence):\n",
    "        for pos in pcfg.lexicon_[word]:\n",
    "            v = pcfg.pos2index[pos]\n",
    "            P[0,s,v] = pcfg.lexicon_[word][pos]\n",
    "    \n",
    "    #browse unaries\n",
    "    for s in range(n):\n",
    "        for prod in pcfg.pcfg_._productions:\n",
    "            if len(prod._rhs)==1:\n",
    "                v1 = pcfg.pos2index[prod._lhs]\n",
    "                v2 = pcfg.pos2index[prod._rhs[0]]\n",
    "                prob_transition = P[0,s,v2] * prod._ProbabilisticMixIn__prob\n",
    "                if P[0,s,v2]>0 and prob_transition > P[0,s,v1]: \n",
    "                    P[0,s,v1] = prob_transition\n",
    "                    back[0,s,v1] = (0,v2,None)\n",
    "                    \n",
    "    #browse binary rules\n",
    "    for l in range(2,n+1): #length of span\n",
    "        for s in range(1,n-l+2): #start of span\n",
    "            for p in range(0,l): #partition of span \n",
    "                for prod in pcfg.pcfg_._productions:\n",
    "                    if len(prod._rhs)==2: #if binary rule\n",
    "                        #print(\"cc\")\n",
    "                        a = pcfg.pos2index[prod._lhs]\n",
    "                        Rb = prod._rhs[0]\n",
    "                        Rc = prod._rhs[1]\n",
    "                        b = pcfg.pos2index[Rb]\n",
    "                        c = pcfg.pos2index[Rc]\n",
    "                        prob_prod = prod._ProbabilisticMixIn__prob\n",
    "                        \n",
    "                        prob_splitting = prob_prod * P[p-1,s-1,b] * P[l-p-1,s+p-1,c]\n",
    "                        if P[p-1,s-1,b] > 0 and P[l-p-1,s+p-1,c] > 0 and P[l-1,s-1,a] < prob_splitting:\n",
    "                            #print(prod)\n",
    "                            P[l-1,s-1,a] = prob_splitting\n",
    "                            back[l-1,s-1,a] = (p,b,c)\n",
    "    \n",
    "    return P,back \n",
    "\n",
    "def build_tree(backp,sentence,pcfg,length=-1,start=0,pos=0): \n",
    "    S = ''\n",
    "    indexes = backp[length,start,pos]\n",
    "    if indexes is None: \n",
    "        return sentence[start]\n",
    "    else: \n",
    "        p,b,c = indexes\n",
    "        if c is None and not b is None: \n",
    "            return \"(\" + pcfg.non_terminals[b]._symbol+ \" \" + sentence[start] + \")\"\n",
    "        #print(non_terminals[pos],non_terminals[b],non_terminals[c])\n",
    "        S += \"(\" +pcfg.non_terminals[b]._symbol+\" \"+ build_tree(backp,sentence,pcfg,p-1,start,b) +\")\"\n",
    "        S += \"(\" +pcfg.non_terminals[c]._symbol+\" \"+ build_tree(backp,sentence,pcfg,length-p,start+p,c) +\")\" \n",
    "            \n",
    "    return S\n",
    "\n",
    "def correct_string(string):\n",
    "    return '( (SENT' + string + '))'\n",
    "\n",
    "def un_chomsky(bracket_string):\n",
    "    \n",
    "    tree = nltk.tree.Tree.fromstring(correct_string(bracket_string))\n",
    "    nltk.treetransforms.un_chomsky_normal_form(tree)\n",
    "    result = tree.pformat().replace('\\n','')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune hypeparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "true_trees = []\n",
    "sentences = []\n",
    "for i,s in enumerate(sentences_test):\n",
    "    if len(s)<15:\n",
    "        true_trees.append(corpus_test[i])\n",
    "        sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (132 of 132) |######################| Elapsed Time: 0:02:16 Time:  0:02:16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136.64666748046875\n"
     ]
    }
   ],
   "source": [
    "new_sentences = []\n",
    "tic = time.time()\n",
    "for l in progressbar.progressbar(range(len(sentences))):\n",
    "    new_sentences.append(oov.get_best_sentence(sentences[l]))\n",
    "print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (132 of 132) |######################| Elapsed Time: 0:03:52 Time:  0:03:52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232.93611574172974\n"
     ]
    }
   ],
   "source": [
    "pred_trees = []\n",
    "tic = time.time()\n",
    "for l in progressbar.progressbar(range(len(sentences))):\n",
    "    P,back  = P_CYK(new_sentences[l], pcfg) \n",
    "    S = un_chomsky(build_tree(back,sentences[l],pcfg))\n",
    "    pred_trees.append(S)\n",
    "print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_not_parsed(s):\n",
    "    c=0\n",
    "    for l in s:\n",
    "        if l==\"(\" or l==\")\":\n",
    "            c+=1\n",
    "    if c==4:\n",
    "        return True\n",
    "    return False      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PYEVALB import parser as evalbparser\n",
    "\n",
    "scorel = []\n",
    "for i in range(len(true_trees)):\n",
    "    if get_not_parsed(pred_trees[i]):\n",
    "        continue\n",
    "    gold = true_trees[i]\n",
    "    test = pred_trees[i]\n",
    "    gold_tree = evalbparser.create_from_bracket_string(gold[1:-1])\n",
    "    test_tree = evalbparser.create_from_bracket_string(test[1:-1])\n",
    "    score=scorer.Scorer()\n",
    "    result = score.score_trees(gold_tree, test_tree)\n",
    "    scorel.append(result.tag_accracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8502234726724525"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scorel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( (SENT (PONCT -) (NP (NPP Gérard) (NPP Longuet))))'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_POS = []\n",
    "for s in prod:\n",
    "    pred_POS.append([rule.lhs() for rule in s if rule.is_lexical()])\n",
    "    \n",
    "true_POS = []\n",
    "for s in true:\n",
    "    t = nltk.tree.Tree.fromstring(s)\n",
    "    p = t.productions()\n",
    "    true_POS.append([rule.lhs() for rule in p if rule.is_lexical()])\n",
    "    \n",
    "c  = 0\n",
    "score = []\n",
    "for p,t in zip(pred_POS,true_POS):\n",
    "    if len(p)==0:\n",
    "        c+=1 \n",
    "        score.append(0)\n",
    "    else:\n",
    "        if len(p)!=len(t):\n",
    "            print(\"alert\")\n",
    "        score.append(np.mean(t==p))\n",
    "print(np.mean(score))\n",
    "\n",
    "c=0\n",
    "for pred in pred_POS:\n",
    "    if len(pred)==0:\n",
    "        c+=1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcfg + lexicon\n",
    "#pcfg = PCFG()\n",
    "#pcfg.fit_pcfg(train)\n",
    "#pcfg.fit_lexicon(train)\n",
    "#pcfg.fit_lexicon(train)\n",
    "#pprint(pcfg.lexicon_)\n",
    "#pcfg.lexicon_['autres']\n",
    "#get_POS('malfaiteurs', pcfg)\n",
    "#get_POS(\"anticoagulants\", pcfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sentence\n",
    "#sentence = sTrain[6]\n",
    "#sentence[0] = 'Ammélioration' # mistake\n",
    "#sentence[0] = \"portiques\" # polyglot but not in train\n",
    "#sentence[-1] = \"pprout\" # polyglot but not in train\n",
    "#print(sentence)\n",
    "#nltk.tree.Tree.fromstring(train[6], remove_empty_top_bracketing=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = 97\n",
    "#print(sTest[n])\n",
    "#nltk.tree.Tree.fromstring(test[n], remove_empty_top_bracketing=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# embeddings\n",
    "#path_emb = \"polyglot-fr.pkl\"\n",
    "#w2v = Word2Vec(path_emb)\n",
    "#print(len(w2v.words),w2v.embeddings.shape)\n",
    "#w2v.extract_subset(vocabTrain)\n",
    "#print(len(w2v.words),w2v.embeddings.shape)\n",
    "#print(w2v.most_similar_embeddings(\"hôtel\"))\n",
    "#print(w2v.most_similar_levenshtein(\"hôtel\", k=10))\n",
    "#print(w2v.most_similar_levenshtein(\"hôtel\",k=10, damerau=True))\n",
    "#w = 'malfaiteurs'\n",
    "#print(w2v.most_similar_embeddings(w))\n",
    "#print(w2v.most_similar_levenshtein(w, k=10))\n",
    "#print(w2v.most_similar_levenshtein(w,k=10, damerau=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_POS('malfaiteurs', pcfg, w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram = Automaton(N=2)\n",
    "#bigram.fit(sTrain)\n",
    "#print(bigram)\n",
    "#bigram.graph[\"*a\"].keys()\n",
    "#bigram = Automaton(N=3)\n",
    "#bigram.fit([[\"banane\",\"baobab\"]])\n",
    "#print(len(bigram.vocab_))\n",
    "#bigram.vocab_\n",
    "#print(bigram)\n",
    "#for w in bigram.vocab_:\n",
    "#    if \"$$\" in w:\n",
    "#        print(w)\n",
    "        \n",
    "#for w in bigram.vocab_:\n",
    "#    if \"*\" in w:\n",
    "#        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from PYEVALB import parser as evalbparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "true_bracket = corpus[3][2:-1]\n",
    "gold_tree = evalbparser.create_from_bracket_string(true_bracket)\n",
    "#test_tree = evalbparser.create_from_bracket_string(proposed_bracket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "nltk.tree.Tree.fromstring(true_bracket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test_bracket = \"(SENT (ADV Ensuite) (PONCT ,) (VN (V fut) (VPP installée)) (NP (DET une) (ADJ autre)) (NC forge) (PP (P à) (NP (DET la) (NPP Vacquerie))) (PONCT ,) (PP (P à) (NP (DET l') (NC emplacement) (ADV aujourd'_hui) (PP (P de) (NP (NPP Cora))))) (PONCT .))\"\n",
    "import nltk\n",
    "nltk.tree.Tree.fromstring(test_bracket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test_tree = evalbparser.create_from_bracket_string(test_bracket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "gold_tree.poss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test_tree.poss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[i==j for i,j in zip(test_tree.poss,gold_tree.poss)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "score=scorer.Scorer()\n",
    "result = score.score_trees(gold_tree, test_tree)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from PYEVALB import scorer\n",
    "from PYEVALB import parser\n",
    "\n",
    "gold = '(IP (NP (PN 这里)) (VP (ADVP (AD 便)) (VP (VV 产生) (IP (NP (QP (CD 一) (CLP (M 个))) (DNP (NP (JJ 结构性)) (DEG 的)) (NP (NN 盲点))) (PU ：) (IP (VP (VV 臭味相投) (PU ，) (VV 物以类聚)))))) (PU 。))'\n",
    "test = '(IP (IP (NP (PN 这里)) (VP (ADVP (AD 便)) (VP (VV 产生) (NP (QP (CD 一) (CLP (M 个))) (DNP (ADJP (JJ 结构性)) (DEG 的)) (NP (NN 盲点)))))) (PU ：) (IP (NP (NN 臭味相投)) (PU ，) (VP (VV 物以类聚))) (PU 。))'\n",
    "\n",
    "gold_tree = parser.create_from_bracket_string(gold)\n",
    "test_tree = parser.create_from_bracket_string(test)\n",
    "\n",
    "score=scorer.Scorer()\n",
    "result = score.score_trees(gold_tree, test_tree)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence():\n",
    "\n",
    "    def __init__(self,sentence):\n",
    "        self.sentence = sentence\n",
    "\n",
    "    def __repr__(self):\n",
    "        print(self.sentence)\n",
    "\n",
    "    def get_tree(self):\n",
    "        \"\"\"build the CNF tree from the nested sentence\"\"\"\n",
    "        tree = nltk.tree.Tree.fromstring(self.clean_sentence,remove_empty_top_bracketing=True)\n",
    "        nltk.treetransforms.chomsky_normal_form(tree)\n",
    "        nltk.treetransforms.collapse_unary(tree,collapsePOS=True)\n",
    "\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grammar():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pcfg = None\n",
    "        self.lexicon = {}\n",
    "        self.productions = []\n",
    "        self.non_terminals = []\n",
    "\n",
    "    def update(self,sentence):\n",
    "\n",
    "        sentence = Sentence(sentence)\n",
    "        tree = sentence.get_tree()\n",
    "        for prod in tree.productions():\n",
    "            if prod.is_lexical():\n",
    "                label = prod._rhs[0]\n",
    "                #append to lexicon\n",
    "                if not label in self.lexicon: #words are the keys for the lexicon\n",
    "                    self.lexicon.update({label:{}})\n",
    "                if not prod._lhs in self.lexicon[label]:\n",
    "                    self.lexicon[label].update({prod._lhs:0})\n",
    "                self.lexicon[label][prod._lhs] += 1\n",
    "\n",
    "\n",
    "            if prod.is_nonlexical():\n",
    "                #append to pcfg\n",
    "                self.productions.append(prod)\n",
    "\n",
    "    def normalize_lexicon(self):\n",
    "        if self.lexicon == {}:\n",
    "            print('Lexicon empty')\n",
    "            return None\n",
    "\n",
    "        for k,d in self.lexicon.items():\n",
    "            somme = sum(d.values())\n",
    "            self.lexicon[k] = {i:v/somme for i,v in d.items()}\n",
    "\n",
    "\n",
    "    def build(self,list_of_sentences):\n",
    "        \"\"\"list_of_sentences est par exemple le set de train\"\"\"\n",
    "        for s in list_of_sentences:\n",
    "            self.update(s)\n",
    "\n",
    "        start = Nonterminal('SENT')\n",
    "        self.pcfg = induce_pcfg(start,self.productions)\n",
    "        self.pcfg.chomsky_normal_form(flexible = False)\n",
    "\n",
    "        #normalize lexicon\n",
    "        self.normalize_lexicon()\n",
    "\n",
    "        #get tokens\n",
    "        for prod in self.pcfg._productions:\n",
    "            for token in prod._rhs:\n",
    "                if not token=='SENT':\n",
    "                    self.non_terminals.append(token)\n",
    "        self.non_terminals.insert(0,start)\n",
    "\n",
    "        #get tokens2index\n",
    "        self.pos2index = {}\n",
    "        for i,token in enumerate(self.non_terminals):\n",
    "            self.pos2index[token] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_CYK(sentence,pcfg, allPOS):\n",
    "    \"\"\"ADAPT CODE AYMERIC !!!\"\"\"\n",
    "    \n",
    "    lexicon = pcfg.lexicon_\n",
    "    #allPOS = list(set(pcfg.POS).union(pcfg.pcfg_.keys()))\n",
    "    pos2index = {p:i for i,p in enumerate(allPOS)}\n",
    "    pcfg = pcfg.pcfg_\n",
    "    \n",
    "    #split_sentence = sentence.split()\n",
    "    \n",
    "    #r = len(grammar.non_terminals) ### ?\n",
    "    r = len(allPOS)\n",
    "    \n",
    "    #n = len(split_sentence)\n",
    "    n = len(sentence)\n",
    "    \n",
    "    \n",
    "    P = np.zeros((n,n,r))\n",
    "    back = np.empty((n,n,r),dtype=np.ndarray)\n",
    "\n",
    "    #for s,word in enumerate(split_sentence):\n",
    "    for s,word in enumerate(sentence):\n",
    "        #for pos in grammar.lexicon[word]:\n",
    "        for pos in lexicon[word]:\n",
    "            #v = grammar.pos2index[pos]\n",
    "            v = pos2index[pos]\n",
    "            #print(word)\n",
    "            #print(lexicon[word])\n",
    "            P[0,s,v] = lexicon[word][pos]\n",
    "            \n",
    "    #return P, allPOS\n",
    "    \n",
    "    #browse unaries -> i do not have any unari !!!\n",
    "    #for s in range(n):\n",
    "    #    for prod in grammar.pcfg._productions:\n",
    "    #        if len(prod._rhs)==1:\n",
    "    #            v1 = grammar.pos2index[prod._lhs]\n",
    "    #            v2 = grammar.pos2index[prod._rhs[0]]\n",
    "    #            prob_transition = P[0,s,v2] * prod._ProbabilisticMixIn__prob\n",
    "    #            if P[0,s,v2]>0 and prob_transition > P[0,s,v1]: \n",
    "    #                P[0,s,v1] = prob_transition\n",
    "    #                back[0,s,v1] = (0,v2,None)\n",
    "                    \n",
    "    #browse binary rules\n",
    "    for l in range(2,n+1): #length of span\n",
    "        for s in range(1,n-l+2): #start of span\n",
    "            for p in range(0,l): #partition of span \n",
    "                #for prod in grammar.pcfg._productions:\n",
    "                for lhs in pcfg:\n",
    "                    for rhs in pcfg[lhs]:\n",
    "                        #if len(prod._rhs)==2: #if binary rule\n",
    "                        #a = grammar.pos2index[prod._lhs]\n",
    "                        a = pos2index[lhs]\n",
    "                        #Rb = prod._rhs[0]\n",
    "                        Rb = rhs[0]\n",
    "                        #Rc = prod._rhs[1]\n",
    "                        Rc = rhs[1]\n",
    "                        #b = grammar.pos2index[Rb]\n",
    "                        b = pos2index[Rb]\n",
    "                        #c = grammar.pos2index[Rc]\n",
    "                        c = pos2index[Rc]\n",
    "                        #prob_prod = prod._ProbabilisticMixIn__prob\n",
    "                        prob_prod = pcfg[lhs][rhs]\n",
    "                        \n",
    "                        prob_splitting = prob_prod * P[p-1,s-1,b] * P[l-p-1,s+p-1,c]\n",
    "                        if P[p-1,s-1,b] > 0 and P[l-p-1,s+p-1,c] > 0 and P[l-1,s-1,a] < prob_splitting:\n",
    "                            #print(prod)\n",
    "                            P[l-1,s-1,a] = prob_splitting\n",
    "                            back[l-1,s-1,a] = (p,b,c)\n",
    "    \n",
    "    return P,back \n",
    "\n",
    "def build_tree(backp,sentence,grammar,length=-1,start=0,pos=0,is_splitted = False): \n",
    "    S = ''\n",
    "    if not is_splitted: \n",
    "        sentence = sentence.split()\n",
    "    indexes = backp[length,start,pos]\n",
    "    if indexes is None: \n",
    "        return sentence[start]\n",
    "    else: \n",
    "        p,b,c = indexes\n",
    "        if c is None and not b is None: \n",
    "            #return \"(\" + grammar.non_terminals[b]._symbol+ \" \" + sentence[start] + \")\"\n",
    "            return \"(\" + grammar[b]+ \" \" + sentence[start] + \")\"\n",
    "        #print(non_terminals[pos],non_terminals[b],non_terminals[c])\n",
    "        #S += \"(\" +grammar.non_terminals[b]._symbol+\" \"+ build_tree(backp,sentence,grammar,p-1,start,b,is_splitted = True) +\")\"\n",
    "        S += \"(\" +grammar[b]+\" \"+ build_tree(backp,sentence,grammar,p-1,start,b,is_splitted = True) +\")\"\n",
    "        #S += \"(\" +grammar.non_terminals[c]._symbol+\" \"+ build_tree(backp,sentence,grammar,length-p,start+p,c,is_splitted = True) +\")\" \n",
    "        S += \"(\" +grammar[c]+\" \"+ build_tree(backp,sentence,grammar,length-p,start+p,c,is_splitted = True) +\")\" \n",
    "            \n",
    "    return S\n",
    "\n",
    "def correct_string(string):\n",
    "    return '( (SENT' + string + '))'\n",
    "\n",
    "def un_chomsky(bracket_string):\n",
    "    \n",
    "    tree = nltk.tree.Tree.fromstring(correct_string(bracket_string))\n",
    "    nltk.treetransforms.un_chomsky_normal_form(tree)\n",
    "    result = tree.pformat().replace('\\n','')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
