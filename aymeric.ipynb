{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence():\n",
    "\n",
    "    def __init__(self,sentence):\n",
    "        self.sentence = sentence\n",
    "\n",
    "    def __repr__(self):\n",
    "        print(self.sentence)\n",
    "\n",
    "    def get_tree(self):\n",
    "        \"\"\"build the CNF tree from the nested sentence\"\"\"\n",
    "        tree = nltk.tree.Tree.fromstring(self.clean_sentence,remove_empty_top_bracketing=True)\n",
    "        nltk.treetransforms.chomsky_normal_form(tree)\n",
    "        nltk.treetransforms.collapse_unary(tree,collapsePOS=True)\n",
    "\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grammar():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pcfg = None\n",
    "        self.lexicon = {}\n",
    "        self.productions = []\n",
    "        self.non_terminals = []\n",
    "\n",
    "    def update(self,sentence):\n",
    "\n",
    "        sentence = Sentence(sentence)\n",
    "        tree = sentence.get_tree()\n",
    "        for prod in tree.productions():\n",
    "            if prod.is_lexical():\n",
    "                label = prod._rhs[0]\n",
    "                #append to lexicon\n",
    "                if not label in self.lexicon: #words are the keys for the lexicon\n",
    "                    self.lexicon.update({label:{}})\n",
    "                if not prod._lhs in self.lexicon[label]:\n",
    "                    self.lexicon[label].update({prod._lhs:0})\n",
    "                self.lexicon[label][prod._lhs] += 1\n",
    "\n",
    "\n",
    "            if prod.is_nonlexical():\n",
    "                #append to pcfg\n",
    "                self.productions.append(prod)\n",
    "\n",
    "    def normalize_lexicon(self):\n",
    "        if self.lexicon == {}:\n",
    "            print('Lexicon empty')\n",
    "            return None\n",
    "\n",
    "        for k,d in self.lexicon.items():\n",
    "            somme = sum(d.values())\n",
    "            self.lexicon[k] = {i:v/somme for i,v in d.items()}\n",
    "\n",
    "\n",
    "    def build(self,list_of_sentences):\n",
    "        \"\"\"list_of_sentences est par exemple le set de train\"\"\"\n",
    "        for s in list_of_sentences:\n",
    "            self.update(s)\n",
    "\n",
    "        start = Nonterminal('SENT')\n",
    "        self.pcfg = induce_pcfg(start,self.productions)\n",
    "        self.pcfg.chomsky_normal_form(flexible = False)\n",
    "\n",
    "        #normalize lexicon\n",
    "        self.normalize_lexicon()\n",
    "\n",
    "        #get tokens\n",
    "        for prod in self.pcfg._productions:\n",
    "            for token in prod._rhs:\n",
    "                if not token=='SENT':\n",
    "                    self.non_terminals.append(token)\n",
    "        self.non_terminals.insert(0,start)\n",
    "\n",
    "        #get tokens2index\n",
    "        self.pos2index = {}\n",
    "        for i,token in enumerate(self.non_terminals):\n",
    "            self.pos2index[token] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_CYK(sentence,grammar):\n",
    "    \n",
    "    split_sentence = sentence.split()\n",
    "    r = len(grammar.non_terminals) \n",
    "    n = len(split_sentence)\n",
    "    P = np.zeros((n,n,r))\n",
    "    back = np.empty((n,n,r),dtype=np.ndarray)\n",
    "\n",
    "    \n",
    "    for s,word in enumerate(split_sentence):\n",
    "        for pos in grammar.lexicon[word]:\n",
    "            v = grammar.pos2index[pos]\n",
    "            P[0,s,v] = grammar.lexicon[word][pos]\n",
    "    \n",
    "    #browse unaries\n",
    "    for s in range(n):\n",
    "        for prod in grammar.pcfg._productions:\n",
    "            if len(prod._rhs)==1:\n",
    "                v1 = grammar.pos2index[prod._lhs]\n",
    "                v2 = grammar.pos2index[prod._rhs[0]]\n",
    "                prob_transition = P[0,s,v2] * prod._ProbabilisticMixIn__prob\n",
    "                if P[0,s,v2]>0 and prob_transition > P[0,s,v1]: \n",
    "                    P[0,s,v1] = prob_transition\n",
    "                    back[0,s,v1] = (0,v2,None)\n",
    "                    \n",
    "    #browse binary rules\n",
    "    for l in range(2,n+1): #length of span\n",
    "        for s in range(1,n-l+2): #start of span\n",
    "            for p in range(0,l): #partition of span \n",
    "                for prod in grammar.pcfg._productions:\n",
    "                    if len(prod._rhs)==2: #if binary rule\n",
    "                        a = grammar.pos2index[prod._lhs]\n",
    "                        Rb = prod._rhs[0]\n",
    "                        Rc = prod._rhs[1]\n",
    "                        b = grammar.pos2index[Rb]\n",
    "                        c = grammar.pos2index[Rc]\n",
    "                        prob_prod = prod._ProbabilisticMixIn__prob\n",
    "                        \n",
    "                        prob_splitting = prob_prod * P[p-1,s-1,b] * P[l-p-1,s+p-1,c]\n",
    "                        if P[p-1,s-1,b] > 0 and P[l-p-1,s+p-1,c] > 0 and P[l-1,s-1,a] < prob_splitting:\n",
    "                            #print(prod)\n",
    "                            P[l-1,s-1,a] = prob_splitting\n",
    "                            back[l-1,s-1,a] = (p,b,c)\n",
    "    \n",
    "    return P,back \n",
    "\n",
    "def build_tree(backp,sentence,grammar,length=-1,start=0,pos=0,is_splitted = False): \n",
    "    S = ''\n",
    "    if not is_splitted: \n",
    "        sentence = sentence.split()\n",
    "    indexes = backp[length,start,pos]\n",
    "    if indexes is None: \n",
    "        return sentence[start]\n",
    "    else: \n",
    "        p,b,c = indexes\n",
    "        if c is None and not b is None: \n",
    "            return \"(\" + grammar.non_terminals[b]._symbol+ \" \" + sentence[start] + \")\"\n",
    "        #print(non_terminals[pos],non_terminals[b],non_terminals[c])\n",
    "        S += \"(\" +grammar.non_terminals[b]._symbol+\" \"+ build_tree(backp,sentence,grammar,p-1,start,b,is_splitted = True) +\")\"\n",
    "        S += \"(\" +grammar.non_terminals[c]._symbol+\" \"+ build_tree(backp,sentence,grammar,length-p,start+p,c,is_splitted = True) +\")\" \n",
    "            \n",
    "    return S\n",
    "\n",
    "def correct_string(string):\n",
    "    return '( (SENT' + string + '))'\n",
    "\n",
    "def un_chomsky(bracket_string):\n",
    "    \n",
    "    tree = nltk.tree.Tree.fromstring(correct_string(bracket_string))\n",
    "    nltk.treetransforms.un_chomsky_normal_form(tree)\n",
    "    result = tree.pformat().replace('\\n','')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_string(string):\n",
    "    return '( (SENT' + string + '))'\n",
    "\n",
    "def un_chomsky(bracket_string):\n",
    "    \n",
    "    tree = nltk.tree.Tree.fromstring(correct_string(bracket_string))\n",
    "    nltk.treetransforms.un_chomsky_normal_form(tree)\n",
    "    result = tree.pformat().replace('\\n','')\n",
    "    return result\n",
    "\n",
    "P,back = P_CYK(new_sentence,pcfg)\n",
    "parsed_sentence = un_chomsky(build_tree(back,sentence,pcfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence,reference):\n",
    "    gold_tree = parser.create_from_bracket_string(sentence[1:-1])\n",
    "    test_tree = parser.create_from_bracket_string(reference[1:-1])\n",
    "\n",
    "    s = Scorer()\n",
    "    result = s.score_trees(gold_tree, test_tree)\n",
    "    \n",
    "    return result.tag_accracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
