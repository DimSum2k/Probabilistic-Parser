{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from PYEVALB import scorer\n",
    "from PYEVALB import parser\n",
    "import operator\n",
    "import pickle\n",
    "import copy\n",
    "#import progressbar\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3099\n",
      "( (SENT (ADV Ensuite) (PONCT ,) (VN (V fut) (VPP installée)) (NP-SUJ (DET une) (ADJ autre) (NC forge)) (PP-P_OBJ (P à) (NP (DET la) (NPP Vacquerie))) (PONCT ,) (PP-P_OBJ (P à) (NP (DET l') (NC emplacement) (ADV aujourd'_hui) (PP (P de) (NP (NPP Cora))))) (PONCT .)))\n",
      "2479 310 310 3099\n"
     ]
    }
   ],
   "source": [
    "#Variables à parser par la suite\n",
    "filepath='HW2/sequoia-corpus+fct.mrg_strict' #'sequoia.txt'\n",
    "\n",
    "#Main\n",
    "with open(filepath, 'r') as f:\n",
    "        corpus = [line.strip('\\n') for line in f]\n",
    "        \n",
    "print(len(corpus))\n",
    "print(corpus[3])\n",
    "        \n",
    "corpus_train=corpus[:2479]\n",
    "corpus_test=corpus[2789:]\n",
    "corpus_dev=corpus[2479:2789]\n",
    "\n",
    "print(len(corpus_train), len(corpus_dev), len(corpus_test), len(corpus_train)+len(corpus_dev)+len(corpus_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusManager:\n",
    "    \"\"\"grammar class\"\"\"\n",
    "\n",
    "    #polyglot-fr.pkl\n",
    "    def __init__(self,corpus):\n",
    "        self.corpus = corpus\n",
    "        self.grammar = {}\n",
    "        self.lexic = {}\n",
    "        self.unigram = {} \n",
    "        self.bigram ={}\n",
    "        self.sentences=[]\n",
    "        self.dic_nodes={}  \n",
    "        self.term_node=[]\n",
    "        self.mat_grammar=[]\n",
    "\n",
    "        \n",
    "    \n",
    "    ###################Tools########################    \n",
    "   \n",
    "    \n",
    "    #Erase the part after the dash in POS\n",
    "    def erase_dash(self):\n",
    "        for i,sent in enumerate(self.corpus):\n",
    "            temp = sent.split('(')\n",
    "            for j,k in enumerate(temp):\n",
    "                if not len(k.split())==0:\n",
    "                    temp[j] =  k.split()[0].split('-')[0]+' '+' '.join(k.split()[1:])\n",
    "            self.corpus[i]='('.join(temp)\n",
    "\n",
    "                  \n",
    "    #Return  a list of words from a bracket sentence\n",
    "    def bracket_to_word(self,sent):\n",
    "        sent = sent[7:]   \n",
    "        words=[]\n",
    "        tmp=sent.split(\")\")\n",
    "        for t in tmp:\n",
    "            try:\n",
    "                words.append(t.split()[-1])\n",
    "            except:\n",
    "                pass\n",
    "        return words\n",
    "     \n",
    "    #build lexic\n",
    "    def build_lexic(self):\n",
    "        lexic={}\n",
    "\n",
    "        for sent in self.corpus:\n",
    "            st=sent.split(\"(\")\n",
    "            for i in range(len(st)):\n",
    "                if \")\" in st[i]:\n",
    "                    l=st[i].replace(\")\",\"\")\n",
    "                    word=l.split()[1]\n",
    "\n",
    "                    tp=l.split()[0]\n",
    "                    tp=tp.split(\"-\")[0]\n",
    "                    if word not in lexic.keys():\n",
    "                        lexic[word] = []\n",
    "                    lexic[word].append(tp)\n",
    "\n",
    "        for key in lexic.keys():\n",
    "                count=Counter(lexic[key])\n",
    "                lexic[key]=dict(count)\n",
    "\n",
    "        for key in lexic.keys():\n",
    "            count=lexic[key]\n",
    "            total = float(sum(count.values()))\n",
    "            for k in count:\n",
    "                count[k] /= total\n",
    "        self.lexic=lexic\n",
    "    \n",
    "    #build bigram values\n",
    "    def build_bigram(self):\n",
    "        \n",
    "        bigrams={}\n",
    "\n",
    "        for u,sentence in enumerate(self.corpus):\n",
    "            sentence=self.bracket_to_word(sentence)\n",
    "            sentence=np.append(sentence,\"#e\")\n",
    "\n",
    "            # \"#s\" and \"#e\" indicate the starting state and the end state\n",
    "            sentence=np.insert(sentence,0,\"#s\")\n",
    "            for i in range(len(sentence)-1):\n",
    "                seq = sentence[i]\n",
    "            \n",
    "                if  seq not in bigrams.keys():\n",
    "                    bigrams[seq] = []\n",
    "                next_chain=sentence[i+1]\n",
    "                \n",
    "                if \"#e\" in next_chain:\n",
    "                    bigrams[seq].append(\"#e\")\n",
    "                else:\n",
    "                    bigrams[seq].append(next_chain)\n",
    "    \n",
    "        #count occurences\n",
    "        for key in bigrams.keys():\n",
    "            bigrams[key]=Counter(bigrams[key])\n",
    "\n",
    "        for key in bigrams.keys():\n",
    "            count=bigrams[key]\n",
    "            total = float(sum(count.values()))\n",
    "            for k in count:\n",
    "                count[k] /= total\n",
    "            bigrams[key]=dict(count)\n",
    "        self.bigram = bigrams\n",
    "         \n",
    "    # build unigram values\n",
    "    def build_unigram(self):\n",
    "        dic_uni={}\n",
    "        for sent in self.corpus:\n",
    "            s=self.bracket_to_word(sent)\n",
    "            for w in s:\n",
    "                if w not in dic_uni.keys():\n",
    "                    dic_uni[w]=1\n",
    "                else:\n",
    "                    dic_uni[w]+=1\n",
    "        \n",
    "       \n",
    "        total = float(sum(dic_uni.values()))\n",
    "        for k,v in dic_uni.items():\n",
    "            dic_uni[k] /= total\n",
    "        self.unigram = dic_uni\n",
    "            \n",
    "    #in order to evaluate on the test\n",
    "    def build_sentences(self):\n",
    "        for s in self.corpus:\n",
    "            self.sentences.append(self.bracket_to_word(s))\n",
    "    \n",
    "    #create a dictionary of indices in order to make the CYK algorithm faster\n",
    "    def build_nodes(self):\n",
    "        list_nodes=[]\n",
    "        for k in self.grammar.keys():\n",
    "            list_nodes.append(k)\n",
    "            for kb in self.grammar[k].keys():\n",
    "                r=kb.split(\"_\")\n",
    "                list_nodes.append(r[0])\n",
    "                if len(r)==2:\n",
    "                    list_nodes.append(r[1])\n",
    "                    \n",
    "        list_nodes=list(set(list_nodes))\n",
    "        self.dic_nodes={n:i for i,n in enumerate(list_nodes)}\n",
    "          \n",
    "\n",
    "    ###################Build PCFG########################\n",
    "    \n",
    "    #build a tree from a bracket sentence\n",
    "    def build_child(self,sentence):\n",
    "        dico={}\n",
    "        sentence_c=sentence[1:-1]\n",
    "        name_node=sentence_c.split()[0]\n",
    "        dico[name_node]={}\n",
    "        u=0\n",
    "        terminal=(sentence_c.find(\"(\")==-1)\n",
    "        if(not terminal):\n",
    "            sentence_c=sentence_c[sentence_c.find(\"(\"):]\n",
    "\n",
    "            while(len(sentence_c.strip())>0):\n",
    "                i=0\n",
    "                find_open=False\n",
    "                while find_open==False:\n",
    "                    if sentence_c[i]==\"(\":\n",
    "                        find_open=True\n",
    "                        start=i\n",
    "                        score=0\n",
    "                        j=i\n",
    "                        find_close_one=False\n",
    "                        while find_close_one==False:\n",
    "                            if sentence_c[j]==\"(\":\n",
    "                                score+=-1\n",
    "                            if sentence_c[j]==\")\":\n",
    "                                score+=1\n",
    "                            if score==0:\n",
    "                                end=j\n",
    "                                find_close_one=True\n",
    "                            j+=1\n",
    "                    i+=1\n",
    "\n",
    "                element_to_remove=\"(\"+sentence_c[start:(end+1)][1:-1]+\")\"\n",
    "                sentence_c=sentence_c[0:start]+sentence_c[(end+1):]\n",
    "                key=str(u)\n",
    "                u=u+1\n",
    "                dico[name_node][name_node+\"_\"+key]=self.build_child(element_to_remove)\n",
    "\n",
    "        else:\n",
    "\n",
    "            v=sentence.split()\n",
    "            sentence=\" \".join(v) \n",
    "            dico=sentence \n",
    "            return(dico)\n",
    "\n",
    "        return(dico)\n",
    "\n",
    "    #add rules on a grammar from the previous tree\n",
    "    def find_child(self,dico,grammar_tree):\n",
    "\n",
    "        for k in dico.keys():\n",
    "            node=k\n",
    "        if node not in grammar_tree.keys():\n",
    "            grammar_tree[node]=[]\n",
    "\n",
    "        list_son=[]\n",
    "        for k, value in dico[node].items():\n",
    "\n",
    "            if type(value) == dict:\n",
    "                for kb in value.keys():\n",
    "                    son=kb\n",
    "                self.find_child(dico[node][k],grammar_tree)\n",
    "            else:\n",
    "                son=value.split()[0][1:]\n",
    "\n",
    "            list_son.append(son)\n",
    "\n",
    "        grammar_tree[node].append(\"_\".join(list_son))\n",
    "       \n",
    "    #build pcfg, add all rules to grammar and normalize to obtain probabilities\n",
    "    def build_pcfg(self):\n",
    "\n",
    "        for sentence in self.corpus:\n",
    "\n",
    "            sentence=sentence[2:(len(sentence)-1)]\n",
    "            self.find_child(self.build_child(sentence),self.grammar)\n",
    "\n",
    "        for k,v in self.grammar.items():\n",
    "            self.grammar[k]=dict(Counter(v))\n",
    "\n",
    "\n",
    "        for k in self.grammar.keys():\n",
    "            count=self.grammar[k]\n",
    "            total = float(sum(count.values()))\n",
    "            for kb in count:\n",
    "                count[kb] /= total\n",
    "            self.grammar[k]=dict(count)\n",
    "    \n",
    "    #create a matrix which entry detailed all the rules in order to gain time in CYK\n",
    "    def build_mat_grammar(self):\n",
    "        \n",
    "        for k,v in self.grammar.items():\n",
    "            for kb, vb in v.items():\n",
    "        \n",
    "                if len(kb.split(\"_\"))==2:\n",
    "                    isunary=False\n",
    "                    #lhs, lrs, prob,lrs1,lrs2,isunary\n",
    "                    self.mat_grammar.append([k,kb,vb,kb.split(\"_\")[0],kb.split(\"_\")[1],isunary])\n",
    "                else:\n",
    "                    isunary=True\n",
    "                    self.mat_grammar.append([k,kb,vb,kb.split(\"_\")[0],None,isunary])\n",
    "\n",
    "     ###################Chomsky Normal Form########################\n",
    "    \n",
    "    #Find terminal node from the grammar in order to compute the chomsky normal form\n",
    "    def find_term_node(self):\n",
    "        term_node=[]\n",
    "        for k,v in self.lexic.items():\n",
    "            for kb in v.keys():\n",
    "                term_node.append(kb)\n",
    "        self.term_node=set(term_node) \n",
    "   \n",
    "    \n",
    "    #Eliminate non-solitary terminals\n",
    "    def non_sol_term(self):\n",
    "        chomsky_tree=copy.deepcopy(self.grammar)\n",
    "        for k,v in self.grammar.items():\n",
    "            for kb, vb in v.items():\n",
    "                tmp=kb.split(\"_\") \n",
    "                if(len(tmp)>1): \n",
    "                    for i,l in enumerate(tmp): \n",
    "                        if l in self.term_node:\n",
    "                            tmp[i]=\"*\"+l \n",
    "                            chomsky_tree[\"*\"+l]={l:1.0} \n",
    "\n",
    "                    del chomsky_tree[k][kb] \n",
    "                    chomsky_tree[k][\"_\".join(tmp)]=vb\n",
    "\n",
    "        self.grammar=chomsky_tree\n",
    "\n",
    "\n",
    "\n",
    "    #Binarization\n",
    "    def binarize(self):\n",
    "        chomsky_tree=copy.deepcopy(self.grammar)\n",
    "        for k,v in self.grammar.items():\n",
    "            for kb, vb in v.items():\n",
    "                tmp=kb.split(\"_\")\n",
    "                nb_split=len(tmp)-1\n",
    "                if(nb_split>1): #if not we may just face an unary node problem for later\n",
    "                    elt=tmp[0]\n",
    "                    #split successively\n",
    "                    for i in range(nb_split):\n",
    "\n",
    "                        new_key=\"|\".join(tmp[i+1:])\n",
    "                        if i !=0:\n",
    "                            chomsky_tree[old_key]={elt+\"_\"+new_key : 1.0} \n",
    "                        else:\n",
    "                            chomsky_tree[k][elt+\"_\"+new_key]=vb\n",
    "                        old_key=new_key\n",
    "                        elt=tmp[i+1]\n",
    "\n",
    "                    del chomsky_tree[k][kb]\n",
    "        self.grammar=chomsky_tree\n",
    "                    \n",
    "\n",
    "    \n",
    "    #Eliminate unary rules with non terminals POS\n",
    "    def unary_rule(self):\n",
    "        chomsky_tree=copy.deepcopy(self.grammar) \n",
    "     \n",
    "        for k,v in self.grammar.items():\n",
    "            #tous les noeuds possibles\n",
    "            for kb in v.keys(): \n",
    "                if (len(kb.split(\"_\"))==1 and kb not in self.term_node):# the rule k->kb  is unary\n",
    "                        prob_trans=chomsky_tree[k][kb] # retain P: A-> B\n",
    "\n",
    "                        del chomsky_tree[k][kb] \n",
    "                        for kt in self.grammar[kb].keys(): \n",
    "\n",
    "                            if chomsky_tree[k].get(kt) != None:\n",
    "                                chomsky_tree[k][kt]+=prob_trans*self.grammar[kb][kt]  \n",
    "                            else:\n",
    "                                chomsky_tree[k][kt]=prob_trans*self.grammar[kb][kt] \n",
    "                                \n",
    "        self.grammar=chomsky_tree\n",
    "    \n",
    "    #Erase unary non terminal rules remaining after the first unarization and renormalize (avoid cycles)\n",
    "    def erase_last_unary(self):\n",
    "        chomsky_tree=copy.deepcopy(self.grammar) \n",
    "        for k, v in self.grammar.items():\n",
    "            normal=False\n",
    "            for kb in v.keys(): \n",
    "                if len(kb.split(\"_\"))==1 and kb not in self.term_node:\n",
    "                    del chomsky_tree[k][kb]\n",
    "                    normal=True\n",
    "            \n",
    "            #Renormalization of child of left hand rule k\n",
    "            if normal:\n",
    "                count=chomsky_tree[k]\n",
    "                total = float(sum(count.values()))\n",
    "                for kb in count:\n",
    "                    count[kb] /= total\n",
    "                    chomsky_tree[k]=dict(count)\n",
    "       \n",
    "        self.grammar=chomsky_tree\n",
    "    \n",
    "    \n",
    "\n",
    "    ###################CYK Algorithm########################\n",
    "   \n",
    "    #Implementation of CYK algorithm with tree probabilities in order to keep the higher probability tree\n",
    "    #Sentence with word in train in entry\n",
    "    def PCYK(self,corrected_sentence):\n",
    "        \n",
    "        n_nodes=len(list(self.dic_nodes.keys()))\n",
    "        n=len(corrected_sentence)\n",
    "        prob = np.zeros((n,n,n_nodes))\n",
    "        back = np.zeros((n,n,n_nodes),dtype=np.ndarray)\n",
    "\n",
    "\n",
    "        for u,word in enumerate(corrected_sentence):\n",
    "            for pos_lexic in self.lexic[word]:\n",
    "                v = self.dic_nodes[pos_lexic]\n",
    "                prob[0,u,v] = self.lexic[word][pos_lexic]\n",
    "    \n",
    "        #Rules which lead to terminals are unary\n",
    "        for u in range(n):\n",
    "            for line_rules in self.mat_grammar:\n",
    "                    lhs, lrs, pr, lrs1,lrs2,isunary = line_rules\n",
    "                    if isunary: \n",
    "                        pos_l = self.dic_nodes[lhs]\n",
    "                        pos_r = self.dic_nodes[lrs]\n",
    "                        prob_unary = prob[0,u,pos_r] * pr \n",
    "                        if prob[0,u,pos_r]>0 and prob_unary > prob[0,u,pos_l]: \n",
    "                            prob[0,u,pos_l] = prob_unary\n",
    "                            back[0,u,pos_l] = (0,pos_r,-1)\n",
    "        \n",
    "        #Binary rules with 2 non terminals\n",
    "        for l in progressbar.progressbar(range(2,n+1)):#length of sub sequence\n",
    "            for u in range(1,n-l+2): #beginning of sub sequence\n",
    "                for w in range(0,l): #where do we split\n",
    "                        for line_rules in self.mat_grammar:\n",
    "                            lhs, lrs, pr, lrs1,lrs2,isunary = line_rules\n",
    "                            if not isunary:\n",
    "                                pos_par = self.dic_nodes[lhs]\n",
    "                                left = self.dic_nodes[lrs1]\n",
    "                                right = self.dic_nodes[lrs2]\n",
    "                                \n",
    "                                prob_prod = pr\n",
    "                                prob_binary = prob_prod * prob[w-1,u-1,left] * prob[l-w-1,u+w-1,right]\n",
    "                               \n",
    "                                if prob[w-1,u-1,left] > 0 and prob[l-w-1,u+w-1,right] > 0 and prob[l-1,u-1,pos_par] < prob_binary:\n",
    "                                    \n",
    "                                    prob[l-1,u-1,pos_par] = prob_binary\n",
    "                                    back[l-1,u-1,pos_par] = (w,left,right)\n",
    "            \n",
    "        return(back, prob)\n",
    "          \n",
    "                            \n",
    "     \n",
    "\n",
    "    # Return the bracket sentence from the CYK back table   \n",
    "    def build_bracket_sent(self,start_sent,back,l,start,pos,S=''): \n",
    "       \n",
    "        indexes = back[l,start,pos]\n",
    "        w,left,right = indexes\n",
    "        if  left!=0 and right == -1 : \n",
    "            return \"(\" + list(self.dic_nodes.keys())[left]+ \" \" + start_sent[start] + \")\"\n",
    "       \n",
    "        S += \"(\" +list(self.dic_nodes.keys())[left]+\" \"+ self.build_bracket_sent(start_sent,back,w-1,start,left) +\")\"\n",
    "        S += \"(\" +list(self.dic_nodes.keys())[right]+\" \"+ self.build_bracket_sent(start_sent,back,l-w,start+w,right) +\")\" \n",
    "\n",
    "        return S\n",
    "    \n",
    "    #unchomsky a bracket sentence obtained from the build_bracket_sent function\n",
    "    def unchomsky_bracket_sentence(self,res_cyk):\n",
    "        while (res_cyk.find(\"(*\")!= -1 or res_cyk.find(\"|\")!=-1):\n",
    "\n",
    "            if res_cyk.find(\"(*\")== -1:\n",
    "                st=res_cyk.find(\"|\")\n",
    "\n",
    "                for j,u in enumerate(reversed(res_cyk[:st])):\n",
    "                    if u ==\"(\":\n",
    "                        break\n",
    "                start=st-(j+1)\n",
    "\n",
    "            else:\n",
    "                start=res_cyk.find(\"(*\")\n",
    "            part=res_cyk[start:]\n",
    "            #ok find the closing parenthesis\n",
    "            score=0\n",
    "            find_close_one=False\n",
    "            i=-1\n",
    "            while not find_close_one:\n",
    "                #print(i)\n",
    "                i=i+1\n",
    "                if (part[i]==\"(\"):\n",
    "                    score+=1\n",
    "                if (part[i]==\")\"):\n",
    "                    score-=1\n",
    "                if score==0:\n",
    "                    find_close_one=True\n",
    "                    end=start+i\n",
    "                    res_cyk=res_cyk[:start]+\" \".join(res_cyk[start:(end+1)].split()[1:])[:-1]+res_cyk[end+1:]\n",
    "            \n",
    "        return(res_cyk)\n",
    "\n",
    "      \n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OOV: \n",
    "    \n",
    "    def __init__(self,lexic_train,path_embeddings,unigram,bigram):\n",
    "        self.lexic = lexic_train\n",
    "        self.bigram = bigram\n",
    "        self.unigram = unigram\n",
    "        words, embeddings = pickle.load(open(path_embeddings, 'rb'),encoding='latin-1')\n",
    "        self.dic_embedding = {k:v for k,v in zip(words,embeddings)}\n",
    "    \n",
    "    #Damerau-Levenshtein distance\n",
    "    def dist_DL(self,w1,w2):\n",
    "        n1 = len(w1)\n",
    "        n2 = len(w2)\n",
    "        m=np.zeros((n1+1,n2+1))\n",
    "\n",
    "        for i  in range(n1+1):\n",
    "            m[i, 0] = i\n",
    "        for j in range(n2+1):\n",
    "            m[0, j] = j\n",
    "        for i in range(1,(n1+1)):\n",
    "            for j in range(1,(n2+1)):\n",
    "                \n",
    "                if w1[i-1] == w2[j-1]:\n",
    "                    cost = 0\n",
    "                else:\n",
    "                    cost = 1\n",
    "\n",
    "                m[i,j] = min(m[i-1,j] + 1,m[i,j-1] + 1,m[i-1,j-1] + cost)\n",
    "\n",
    "                if( w1[i-1] == w2[j-2] and w1[i-2] == w2[j-1]): #Transposition\n",
    "                    m[i,j] = min(m[i,j],m[i-2,j-2] + cost) \n",
    "\n",
    "        return(m[n1,n2])\n",
    "\n",
    "    #Cosine similarity\n",
    "    def dist_cosine(self,w1,w2):\n",
    "        a = self.dic_embedding[w1]\n",
    "        b = self.dic_embedding[w2]\n",
    "        return (np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b)))\n",
    "\n",
    "    #Return n nearest neighbour computed with cosine similarity or DL \n",
    "    def nearest_neighbour(self,word,n,dist=\"dist_cosine\"):\n",
    "       \n",
    "        if dist==\"dist_cosine\":\n",
    "            distances = {e:self.dist_cosine(word,e) for e in self.lexic.keys() if e in self.dic_embedding.keys() }\n",
    "            sorted_distances = sorted(distances.items(), key=operator.itemgetter(1))[::-1]\n",
    "        else:\n",
    "            distances = {e:self.dist_DL(word,e) for e in self.lexic.keys()}\n",
    "            sorted_distances = sorted(distances.items(), key=operator.itemgetter(1))   \n",
    "  \n",
    "        return [s[0] for s in sorted_distances[:n]]\n",
    "    \n",
    "    #Return n_cosine nearest neighbour from dist cosine and n_lev nearest neighbour from DL distance\n",
    "    def give_candidates_train(self,word,n_cosine,n_lev):\n",
    "    \n",
    "        list_cosine=[]\n",
    "        if word in self.dic_embedding.keys():\n",
    "\n",
    "            list_cosine=self.nearest_neighbour(word,n_cosine,dist=\"dist_cosine\")\n",
    "\n",
    "        list_levenstein=self.nearest_neighbour(word,n_lev,dist=\"Lev\")\n",
    "\n",
    "        return(list_cosine + list_levenstein) \n",
    "    \n",
    "    #Process the whole sentence usingcandidates from the distances, unigram and bigram\n",
    "    #Return a \"word-in-train\" sentence in order to feed PCYK algorithm\n",
    "    def process_a_sentence(self,sent,n_cos,n_lev,hyper_lambda):\n",
    "        sent_in_train=[]\n",
    "        for i,word in enumerate(sent):\n",
    "            if word in self.lexic.keys():\n",
    "                sent_in_train.append(word) \n",
    "            else:  \n",
    "                pmax=0\n",
    "                for candidate in self.give_candidates_train(word,n_cos,n_lev):\n",
    "                   \n",
    "                    if i==0:\n",
    "                        start=\"#s\"\n",
    "                    else:\n",
    "                        start=sent_in_train[i-1] \n",
    "\n",
    "                    if oov.bigram[start].get(candidate)!= None:  \n",
    "                        p= hyper_lambda * self.unigram[candidate]+(1-hyper_lambda)*self.bigram[start][candidate]\n",
    "                    else:\n",
    "                        p= hyper_lambda * self.unigram[candidate]\n",
    "\n",
    "                    if p>pmax:\n",
    "                        pmax=p\n",
    "                        winner=candidate\n",
    "\n",
    "                sent_in_train.append(winner)\n",
    "       \n",
    "        return(sent_in_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=CorpusManager(corpus_train)\n",
    "train.erase_dash()\n",
    "#print(train.corpus[3])\n",
    "train.build_pcfg()\n",
    "#pprint(train.grammar)\n",
    "train.build_lexic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gutenberg': {'NPP': 1.0},\n",
       " 'Cette': {'DET': 1.0},\n",
       " 'exposition': {'NC': 1.0},\n",
       " 'nous': {'CLO': 0.11450381679389313,\n",
       "  'CLS': 0.8015267175572519,\n",
       "  'CLR': 0.06870229007633588,\n",
       "  'PRO': 0.015267175572519083},\n",
       " 'apprend': {'V': 1.0},\n",
       " 'que': {'CS': 0.7723342939481268,\n",
       "  'PROREL': 0.1786743515850144,\n",
       "  'ADV': 0.04899135446685879},\n",
       " 'dès': {'P': 1.0},\n",
       " 'le': {'DET': 0.9590062111801242, 'CLO': 0.040993788819875775},\n",
       " 'XIIe': {'ADJ': 1.0},\n",
       " 'siècle': {'NC': 1.0},\n",
       " ',': {'PONCT': 1.0},\n",
       " 'à': {'P': 1.0},\n",
       " 'Dammarie-sur-Saulx': {'NPP': 1.0},\n",
       " 'entre': {'P': 1.0},\n",
       " 'autres': {'ADJ': 0.972972972972973, 'PRO': 0.02702702702702703},\n",
       " 'sites': {'NC': 1.0},\n",
       " 'une': {'DET': 0.989010989010989, 'PRO': 0.01098901098901099},\n",
       " 'industrie': {'NC': 1.0},\n",
       " 'métallurgique': {'ADJ': 1.0},\n",
       " 'existait': {'V': 1.0},\n",
       " '.': {'PONCT': 1.0},\n",
       " 'à_peu_près': {'ADV': 1.0},\n",
       " 'au': {'P+D': 1.0},\n",
       " 'même': {'ADJ': 0.6774193548387096, 'ADV': 0.3225806451612903},\n",
       " 'moment': {'NC': 1.0},\n",
       " 'inventait': {'V': 1.0},\n",
       " \"l'\": {'DET': 0.9700598802395209, 'CLO': 0.029940119760479042},\n",
       " 'imprimerie': {'NC': 1.0},\n",
       " 'Gillet': {'NPP': 1.0},\n",
       " 'Bonnemire': {'NPP': 1.0},\n",
       " 'créait': {'V': 1.0},\n",
       " 'en': {'P': 0.8611898016997167,\n",
       "  'CLO': 0.12464589235127478,\n",
       "  'ET': 0.014164305949008499},\n",
       " '1450': {'NC': 1.0},\n",
       " 'la': {'DET': 0.9910380881254668, 'CLO': 0.008961911874533234},\n",
       " 'première': {'ADJ': 0.9583333333333334, 'NC': 0.041666666666666664},\n",
       " 'forge': {'NC': 1.0},\n",
       " 'Saint-Dizier': {'NPP': 1.0},\n",
       " 'actuel': {'ADJ': 1.0},\n",
       " 'emplacement': {'NC': 1.0},\n",
       " 'du': {'P+D': 0.9655172413793104,\n",
       "  'VPP': 0.0021551724137931034,\n",
       "  'DET': 0.032327586206896554},\n",
       " 'CHS': {'NPP': 1.0},\n",
       " 'Ensuite': {'ADV': 1.0},\n",
       " 'fut': {'V': 1.0},\n",
       " 'installée': {'VPP': 1.0},\n",
       " 'autre': {'ADJ': 0.9117647058823529, 'PRO': 0.08823529411764706},\n",
       " 'Vacquerie': {'NPP': 1.0},\n",
       " \"aujourd'_hui\": {'ADV': 1.0},\n",
       " 'de': {'P': 0.9788445890968267, 'DET': 0.02115541090317331},\n",
       " 'Cora': {'NPP': 1.0},\n",
       " 'En': {'P': 1.0},\n",
       " '1953': {'NC': 1.0},\n",
       " 'les': {'DET': 0.9799247176913425, 'CLO': 0.020075282308657464},\n",
       " 'hauts': {'ADJ': 0.5, 'NC': 0.5},\n",
       " 'fourneaux': {'NC': 1.0},\n",
       " 'et': {'CC': 1.0},\n",
       " 'fonderies': {'NC': 1.0},\n",
       " 'Cousances': {'NPP': 1.0},\n",
       " 'virent': {'V': 1.0},\n",
       " 'jour': {'NC': 1.0},\n",
       " 'puis': {'CC': 0.8, 'V': 0.2},\n",
       " 'Jean': {'NPP': 1.0},\n",
       " 'Baudesson': {'NPP': 1.0},\n",
       " 'maire': {'NC': 1.0},\n",
       " 'échevin': {'NC': 1.0},\n",
       " 'autorisé': {'VPP': 1.0},\n",
       " 'par': {'P': 1.0},\n",
       " 'lettres': {'NC': 1.0},\n",
       " 'patentes': {'ADJ': 1.0},\n",
       " \"d'\": {'P': 0.9807427785419532,\n",
       "  'DET': 0.017881705639614855,\n",
       "  'P+D': 0.001375515818431912},\n",
       " 'Henri': {'NPP': 1.0},\n",
       " 'IV': {'ADJ': 0.5, 'NC': 0.5},\n",
       " 'installa': {'V': 1.0},\n",
       " 'Marnaval': {'NPP': 1.0},\n",
       " '-': {'PONCT': 0.9783783783783784,\n",
       "  'CC': 0.010810810810810811,\n",
       "  'ADV': 0.010810810810810811},\n",
       " 'qui': {'PROREL': 0.9929577464788732, 'PROWH': 0.007042253521126761},\n",
       " 'signifiait': {'V': 1.0},\n",
       " 'val': {'NC': 1.0},\n",
       " 'ou': {'CC': 1.0},\n",
       " 'vallée': {'NC': 1.0},\n",
       " 'Marne': {'NPP': 1.0},\n",
       " 'ou_bien': {'CC': 1.0},\n",
       " 'aval': {'NC': 1.0},\n",
       " 'connut': {'V': 1.0},\n",
       " 'son': {'DET': 1.0},\n",
       " 'apogée': {'NC': 1.0},\n",
       " 'XIXe': {'ADJ': 1.0},\n",
       " 'Tout_au_long_des': {'P+D': 1.0},\n",
       " 'années': {'NC': 1.0},\n",
       " 'nouveaux': {'ADJ': 1.0},\n",
       " 'furent': {'V': 1.0},\n",
       " 'créés': {'VPP': 1.0},\n",
       " 'notamment': {'ADV': 1.0},\n",
       " 'Haironville': {'NPP': 1.0},\n",
       " 'plus_tard': {'ADV': 1.0},\n",
       " 'Ancerville': {'NPP': 1.0},\n",
       " 'Amélioration': {'NC': 1.0},\n",
       " 'sécurité': {'NC': 1.0},\n",
       " 'Le': {'DET': 0.993421052631579, 'NPP': 0.006578947368421052},\n",
       " 'a': {'V': 0.9968944099378882, 'ET': 0.003105590062111801},\n",
       " 'invité': {'VPP': 1.0},\n",
       " 'membres': {'NC': 0.7878787878787878, 'ADJ': 0.21212121212121213},\n",
       " 'conseil': {'NC': 1.0},\n",
       " 'élaborer': {'VINF': 1.0},\n",
       " 'programme': {'NC': 1.0},\n",
       " 'amélioration': {'NC': 1.0},\n",
       " 'voirie': {'NC': 1.0},\n",
       " 'communale': {'ADJ': 1.0},\n",
       " 'routière': {'ADJ': 1.0},\n",
       " 'pour': {'P': 1.0},\n",
       " 'année': {'NC': 1.0},\n",
       " '1999': {'NC': 1.0},\n",
       " 'Il': {'CLS': 1.0},\n",
       " 'rappelé': {'VPP': 1.0},\n",
       " 'plusieurs': {'DET': 0.9583333333333334, 'PRO': 0.041666666666666664},\n",
       " 'automobilistes': {'NC': 1.0},\n",
       " 'ont': {'V': 1.0},\n",
       " 'quitté': {'VPP': 1.0},\n",
       " 'chaussée': {'NC': 1.0},\n",
       " 'intersection': {'NC': 1.0},\n",
       " 'RD192': {'NC': 1.0},\n",
       " 'chemin': {'NC': 1.0},\n",
       " 'rural': {'ADJ': 1.0},\n",
       " 'Vaux': {'NPP': 1.0},\n",
       " 'des': {'P+D': 0.6523929471032746, 'DET': 0.34760705289672544},\n",
       " 'Fossés': {'NPP': 1.0},\n",
       " \"qu'\": {'CS': 0.6776859504132231,\n",
       "  'ADV': 0.06611570247933884,\n",
       "  'PROREL': 0.256198347107438},\n",
       " 'il': {'CLS': 1.0},\n",
       " 'convient': {'V': 1.0},\n",
       " 'modifier': {'VINF': 1.0},\n",
       " 'régime': {'NC': 1.0},\n",
       " 'priorité': {'NC': 1.0},\n",
       " 'cet': {'DET': 1.0},\n",
       " 'endroit': {'NC': 1.0},\n",
       " 'La': {'DET': 1.0},\n",
       " 'pose': {'NC': 0.2222222222222222, 'V': 0.7777777777777778},\n",
       " 'un': {'DET': 0.9379014989293362, 'PRO': 0.06209850107066381},\n",
       " 'panneau': {'NC': 1.0},\n",
       " 'stop': {'NC': 1.0},\n",
       " 'paraît': {'V': 1.0},\n",
       " 'être': {'VINF': 0.9808917197452229, 'NC': 0.01910828025477707},\n",
       " 'formule': {'NC': 1.0},\n",
       " 'mieux': {'ADV': 0.8888888888888888, 'NC': 0.1111111111111111},\n",
       " 'adaptée': {'ADJ': 0.3333333333333333, 'VPP': 0.6666666666666666},\n",
       " 'assurer': {'VINF': 1.0},\n",
       " 'usagers': {'NC': 1.0},\n",
       " 'délibérant': {'VPR': 1.0},\n",
       " 'assemblée': {'NC': 1.0},\n",
       " 'accepté': {'VPP': 1.0},\n",
       " 'proposition': {'NC': 1.0},\n",
       " 'chargé': {'VPP': 1.0},\n",
       " 'faire': {'VINF': 1.0},\n",
       " 'établir': {'VINF': 1.0},\n",
       " 'services': {'NC': 1.0},\n",
       " 'DDE': {'NPP': 1.0},\n",
       " 'dossier': {'NC': 1.0},\n",
       " 'demande': {'NC': 0.5294117647058824, 'V': 0.47058823529411764},\n",
       " 'subvention': {'NC': 1.0},\n",
       " 'dans_le_cadre_de': {'P': 1.0},\n",
       " 'répartition': {'NC': 1.0},\n",
       " 'amendes': {'NC': 1.0},\n",
       " 'police': {'NC': 1.0},\n",
       " 'Espoir': {'NC': 1.0},\n",
       " 'remise': {'NC': 1.0},\n",
       " 'lavoir': {'NC': 1.0},\n",
       " 'abrite': {'V': 1.0},\n",
       " 'depuis': {'P': 1.0},\n",
       " 'quelques': {'DET': 0.9166666666666666, 'ADJ': 0.08333333333333333},\n",
       " 'jours': {'NC': 1.0},\n",
       " 'été': {'VPP': 0.9951923076923077, 'NC': 0.004807692307692308},\n",
       " 'inaugurée': {'VPP': 1.0},\n",
       " 'Roger': {'NPP': 1.0},\n",
       " 'Thiriot': {'NPP': 1.0},\n",
       " 'président': {'NC': 1.0},\n",
       " 'Association': {'NC': 0.4, 'NPP': 0.6},\n",
       " 'insertion': {'NC': 1.0},\n",
       " 'pays': {'NC': 1.0},\n",
       " 'Saulx': {'NPP': 1.0},\n",
       " 'Perthois': {'NPP': 1.0},\n",
       " 'en_présence_de': {'P': 1.0},\n",
       " 'nombreux': {'ADJ': 1.0},\n",
       " 'élus': {'NC': 1.0},\n",
       " 'canton': {'NC': 1.0},\n",
       " \"L'\": {'DET': 1.0},\n",
       " 'association': {'NC': 1.0},\n",
       " 'changé': {'VPP': 1.0},\n",
       " 'décors': {'NC': 1.0},\n",
       " 'avec': {'P': 1.0},\n",
       " 'aide': {'NC': 0.8823529411764706, 'V': 0.11764705882352941},\n",
       " 'bénévoles': {'NC': 1.0},\n",
       " 'établi': {'VPP': 1.0},\n",
       " 'différents': {'ADJ': 1.0},\n",
       " 'tableaux': {'NC': 1.0},\n",
       " 'sur': {'P': 1.0},\n",
       " 'thème': {'NC': 1.0},\n",
       " '\"': {'PONCT': 1.0},\n",
       " 'Cinq': {'DET': 1.0},\n",
       " 'siècles': {'NC': 1.0},\n",
       " 'activité': {'NC': 1.0},\n",
       " 'économique': {'ADJ': 1.0},\n",
       " 'région': {'NC': 1.0},\n",
       " 'Pourquoi': {'ADVWH': 1.0},\n",
       " 'ce': {'DET': 0.6549295774647887,\n",
       "  'PRO': 0.30985915492957744,\n",
       "  'CLS': 0.035211267605633804},\n",
       " '?': {'PONCT': 1.0},\n",
       " 'Tout': {'ADV': 0.3333333333333333,\n",
       "  'ADJ': 0.3333333333333333,\n",
       "  'DET': 0.3333333333333333},\n",
       " 'simplement': {'ADV': 1.0},\n",
       " 'précisé': {'VPP': 1.0},\n",
       " 'parce_que': {'CS': 1.0},\n",
       " 'histoire': {'NC': 1.0},\n",
       " 'travail': {'NC': 1.0},\n",
       " 'industriel': {'ADJ': 1.0},\n",
       " 'est': {'V': 1.0},\n",
       " 'ici': {'ADV': 1.0},\n",
       " 'longue': {'ADJ': 1.0},\n",
       " 'vieille': {'ADJ': 1.0},\n",
       " 'présence': {'NC': 1.0},\n",
       " 'abondante': {'ADJ': 1.0},\n",
       " 'forêt': {'NC': 1.0},\n",
       " 'fournissant': {'VPR': 1.0},\n",
       " 'indispensable': {'ADJ': 1.0},\n",
       " 'combustible': {'NC': 1.0},\n",
       " 'liée': {'VPP': 1.0},\n",
       " 'existence': {'NC': 1.0},\n",
       " 'minerai': {'NC': 1.0},\n",
       " 'fer': {'NC': 1.0},\n",
       " 'exploitable': {'ADJ': 1.0},\n",
       " 'ciel': {'NC': 1.0},\n",
       " 'ouvert': {'ADJ': 0.6, 'VPP': 0.4},\n",
       " 'incité': {'VPP': 1.0},\n",
       " 'hommes': {'NC': 1.0},\n",
       " 'créer': {'VINF': 1.0},\n",
       " \"C'\": {'CLS': 1.0},\n",
       " 'donc': {'ADV': 1.0},\n",
       " 'toute': {'ADJ': 0.5909090909090909,\n",
       "  'ADV': 0.09090909090909091,\n",
       "  'DET': 0.3181818181818182},\n",
       " 'vie': {'NC': 1.0},\n",
       " 'industrielle': {'ADJ': 1.0},\n",
       " 'bassin': {'NC': 1.0},\n",
       " 'sans': {'P': 1.0},\n",
       " 'oublier': {'VINF': 1.0},\n",
       " 'papeteries': {'NC': 1.0},\n",
       " \"Jeand'Heurs\": {'NPP': 1.0},\n",
       " 'carrières': {'NC': 1.0},\n",
       " 'Savonnières': {'NPP': 1.0},\n",
       " 'visiteurs': {'NC': 1.0},\n",
       " 'pourront': {'V': 1.0},\n",
       " 'découvrir': {'VINF': 1.0},\n",
       " 'Un': {'DET': 1.0},\n",
       " 'voyage': {'NC': 1.0},\n",
       " 'étonnant': {'ADJ': 1.0},\n",
       " 'où': {'PROREL': 0.9444444444444444, 'ADVWH': 0.05555555555555555},\n",
       " 'photos': {'NC': 1.0},\n",
       " 'réalisations': {'NC': 1.0},\n",
       " 'vieux': {'ADJ': 1.0},\n",
       " 'outils': {'NC': 1.0},\n",
       " 'documents': {'NC': 1.0},\n",
       " 'anciens': {'ADJ': 0.875, 'NC': 0.125},\n",
       " 'permettront': {'V': 1.0},\n",
       " 'mesurer': {'VINF': 1.0},\n",
       " 'combien': {'ADVWH': 1.0},\n",
       " 'petit': {'ADJ': 1.0},\n",
       " 'bout': {'NC': 1.0},\n",
       " 'terre': {'NC': 1.0},\n",
       " 'France': {'NPP': 1.0},\n",
       " 'situé': {'VPP': 1.0},\n",
       " 'aux': {'P+D': 1.0},\n",
       " 'Marches': {'NPP': 1.0},\n",
       " 'Est': {'NC': 0.5, 'NPP': 0.5},\n",
       " 'lieu': {'NC': 1.0},\n",
       " 'passage': {'NC': 1.0},\n",
       " 'prédilection': {'NC': 1.0},\n",
       " 'invasions': {'NC': 1.0},\n",
       " 'labeur': {'NC': 1.0},\n",
       " 'ses': {'DET': 1.0},\n",
       " 'habitants': {'NC': 1.0},\n",
       " 'durs': {'ADJ': 1.0},\n",
       " 'tâche': {'NC': 1.0},\n",
       " 'doit': {'V': 1.0},\n",
       " 'reconnu': {'VPP': 1.0},\n",
       " 'mis': {'VPP': 1.0},\n",
       " 'valeur': {'NC': 1.0},\n",
       " 'comme': {'CS': 0.35443037974683544, 'P': 0.6455696202531646},\n",
       " 'devait': {'V': 1.0},\n",
       " 'conclure': {'VINF': 1.0},\n",
       " \"n'\": {'ADV': 1.0},\n",
       " 'ambition': {'NC': 1.0},\n",
       " 'apporter': {'VINF': 1.0},\n",
       " 'modeste': {'ADJ': 1.0},\n",
       " 'témoignage': {'NC': 1.0},\n",
       " 'passé': {'NC': 0.42857142857142855, 'VPP': 0.5714285714285714},\n",
       " 'tissu': {'NC': 1.0},\n",
       " 'Elle': {'CLS': 1.0},\n",
       " 'se': {'CLR': 1.0},\n",
       " 'veut': {'V': 1.0},\n",
       " 'aussi': {'ADV': 1.0},\n",
       " 'message': {'NC': 1.0},\n",
       " 'espoir': {'NC': 1.0},\n",
       " 'avenir': {'NC': 1.0},\n",
       " 'Ouverture': {'NC': 1.0},\n",
       " 'tous': {'ADJ': 0.723404255319149,\n",
       "  'PRO': 0.2127659574468085,\n",
       "  'DET': 0.06382978723404255},\n",
       " 'sauf': {'P': 1.0},\n",
       " 'lundi': {'NC': 1.0},\n",
       " '14': {'DET': 0.8888888888888888, 'ADJ': 0.1111111111111111},\n",
       " 'h': {'NC': 1.0},\n",
       " '30': {'ADJ': 0.27586206896551724, 'DET': 0.7241379310344828},\n",
       " '18': {'DET': 0.7, 'ADJ': 0.3},\n",
       " 'Une': {'DET': 1.0},\n",
       " 'nouvelle': {'ADJ': 0.9473684210526315, 'NC': 0.05263157894736842},\n",
       " 'école': {'NC': 1.0},\n",
       " 'commune': {'NC': 0.6842105263157895, 'ADJ': 0.3157894736842105},\n",
       " 'Au_cours_de': {'P': 1.0},\n",
       " 'cérémonie': {'NC': 1.0},\n",
       " 'inauguration': {'NC': 1.0},\n",
       " 'maternelle': {'ADJ': 1.0},\n",
       " 'en_présence_du': {'P+D': 1.0},\n",
       " 'sous-préfet': {'NC': 1.0},\n",
       " 'général': {'ADJ': 0.45454545454545453, 'NC': 0.5454545454545454},\n",
       " 'accompagné': {'VPP': 1.0},\n",
       " 'MM.': {'NC': 1.0},\n",
       " 'Farinet': {'NPP': 1.0},\n",
       " 'Verneau': {'NPP': 1.0},\n",
       " 'maires': {'NC': 1.0},\n",
       " 'représentants': {'NC': 1.0},\n",
       " 'communauté': {'NC': 1.0},\n",
       " 'communes': {'NC': 1.0},\n",
       " 'inspecteur': {'NC': 1.0},\n",
       " 'académie': {'NC': 1.0},\n",
       " 'inspectrice': {'NC': 1.0},\n",
       " 'Education': {'NC': 1.0},\n",
       " 'nationale': {'ADJ': 1.0},\n",
       " 'circonscription': {'NC': 1.0},\n",
       " 'Après': {'P': 1.0},\n",
       " 'avoir': {'VINF': 1.0},\n",
       " 'coupé': {'VPP': 1.0},\n",
       " 'ruban': {'NC': 1.0},\n",
       " 'marque': {'V': 0.25, 'NC': 0.75},\n",
       " 'symboliquement': {'ADV': 1.0},\n",
       " 'entrée': {'NC': 1.0},\n",
       " 'directrice': {'NC': 0.6666666666666666, 'ADJ': 0.3333333333333333},\n",
       " 'conduit': {'VPP': 1.0},\n",
       " 'visite': {'NC': 1.0},\n",
       " 'commentée': {'VPP': 1.0},\n",
       " 'locaux': {'NC': 1.0},\n",
       " 'Dans': {'P': 1.0},\n",
       " 'intervention': {'NC': 1.0},\n",
       " 'M.': {'NC': 1.0},\n",
       " 'Soyer': {'NPP': 1.0},\n",
       " 'fait': {'VPP': 0.5208333333333334,\n",
       "  'V': 0.2708333333333333,\n",
       "  'NC': 0.20833333333333334},\n",
       " 'historique': {'NC': 1.0},\n",
       " 'existe': {'V': 1.0},\n",
       " 'Vignot': {'NPP': 1.0},\n",
       " '1972': {'NC': 1.0},\n",
       " 'occupait': {'V': 1.0},\n",
       " 'cette': {'DET': 1.0},\n",
       " 'date': {'NC': 0.8, 'V': 0.1, 'ET': 0.1},\n",
       " 'bâtiment': {'NC': 1.0},\n",
       " 'préfabriqué': {'VPP': 1.0},\n",
       " 'dans': {'P': 1.0},\n",
       " 'parc': {'NC': 1.0},\n",
       " '1996': {'NC': 1.0},\n",
       " 'municipalité': {'NC': 1.0},\n",
       " 'étudie': {'V': 1.0},\n",
       " 'possibilité': {'NC': 1.0},\n",
       " 'construction': {'NC': 1.0},\n",
       " 'neuve': {'ADJ': 1.0},\n",
       " 'réflexion': {'NC': 1.0},\n",
       " 'menée': {'VPP': 1.0},\n",
       " 'enseignants': {'NC': 1.0},\n",
       " 'délégués': {'NC': 0.8, 'VPP': 0.2},\n",
       " 'parents': {'NC': 1.0},\n",
       " 'élèves': {'NC': 1.0},\n",
       " 'sous': {'P': 1.0},\n",
       " 'conduite': {'NC': 0.7777777777777778, 'VPP': 0.2222222222222222},\n",
       " 'CAUE': {'NPP': 1.0},\n",
       " 'études': {'NC': 1.0},\n",
       " 'travaux': {'NC': 1.0},\n",
       " 'cabinet': {'NC': 1.0},\n",
       " 'Cadel': {'NPP': 1.0},\n",
       " 'associé': {'NC': 0.14285714285714285, 'VPP': 0.8571428571428571},\n",
       " 'Bruand': {'NPP': 1.0},\n",
       " 'contrôle': {'NC': 1.0},\n",
       " 'Equipement': {'NC': 1.0},\n",
       " 'neuf': {'DET': 0.5, 'ADJ': 0.5},\n",
       " 'mois': {'NC': 1.0},\n",
       " 'coût': {'NC': 1.0},\n",
       " 'bâtiments': {'NC': 1.0},\n",
       " \"s'\": {'CLR': 0.9622641509433962, 'CS': 0.03773584905660377},\n",
       " 'élève': {'V': 1.0},\n",
       " '2.700.000': {'DET': 1.0},\n",
       " 'F': {'NC': 1.0},\n",
       " 'dont': {'PROREL': 1.0},\n",
       " '40': {'DET': 0.6666666666666666,\n",
       "  'PRO': 0.16666666666666666,\n",
       "  'ADJ': 0.16666666666666666},\n",
       " '%': {'NC': 1.0},\n",
       " 'Etat': {'NC': 1.0},\n",
       " ';': {'PONCT': 1.0},\n",
       " '20': {'DET': 0.7272727272727273, 'ADJ': 0.2727272727272727},\n",
       " 'auto-': {'PREF': 1.0},\n",
       " 'financement': {'NC': 1.0},\n",
       " 'emprunt': {'NC': 1.0},\n",
       " 'mobilier': {'NC': 1.0},\n",
       " 'subventionné': {'VPP': 1.0},\n",
       " '50': {'DET': 0.7777777777777778,\n",
       "  'PRO': 0.1111111111111111,\n",
       "  'ADJ': 0.1111111111111111},\n",
       " 'adressé': {'VPP': 1.0},\n",
       " 'remerciements': {'NC': 1.0},\n",
       " 'collectivités': {'NC': 1.0},\n",
       " 'participantes': {'ADJ': 1.0},\n",
       " 'en_particulier': {'ADV': 1.0},\n",
       " 'création': {'NC': 1.0},\n",
       " '1998': {'NC': 1.0},\n",
       " 'cinquième': {'ADJ': 1.0},\n",
       " 'classe': {'NC': 1.0},\n",
       " 'nomination': {'NC': 1.0},\n",
       " 'deux': {'DET': 0.647887323943662,\n",
       "  'ADJ': 0.30985915492957744,\n",
       "  'PRO': 0.04225352112676056},\n",
       " 'aides-éducateurs': {'NC': 1.0},\n",
       " 'mise': {'NC': 0.8846153846153846, 'VPP': 0.11538461538461539},\n",
       " 'disposition': {'NC': 1.0},\n",
       " 'rentrée': {'NC': 1.0},\n",
       " 'titulaire': {'NC': 1.0},\n",
       " 'mobile': {'ADJ': 1.0},\n",
       " 'Dumez': {'NPP': 1.0},\n",
       " 'parlé': {'VPP': 1.0},\n",
       " 'réalisation': {'NC': 1.0},\n",
       " 'apte': {'ADJ': 1.0},\n",
       " 'épanouissement': {'NC': 1.0},\n",
       " 'petits': {'NC': 0.1111111111111111, 'ADJ': 0.8888888888888888},\n",
       " 'non': {'ADV': 0.9722222222222222, 'NC': 0.027777777777777776},\n",
       " 'rappeler': {'VINF': 1.0},\n",
       " 'rôle': {'NC': 1.0},\n",
       " 'primordial': {'ADJ': 1.0},\n",
       " 'éducation': {'NC': 1.0},\n",
       " 'familiale': {'ADJ': 1.0},\n",
       " 'remercié': {'VPP': 1.0},\n",
       " 'mené_à_bien': {'VPP': 1.0},\n",
       " 'projet': {'NC': 1.0},\n",
       " 'ayant': {'VPR': 1.0},\n",
       " 'différentes': {'ADJ': 0.8333333333333334, 'DET': 0.16666666666666666},\n",
       " 'parties': {'NC': 1.0},\n",
       " 'intéressées': {'ADJ': 1.0},\n",
       " ':': {'PONCT': 1.0},\n",
       " 'conception': {'NC': 1.0},\n",
       " 'qualité': {'NC': 1.0},\n",
       " 'matériaux': {'NC': 1.0},\n",
       " 'utilisés': {'VPP': 1.0},\n",
       " 'Quant_au': {'P+D': 1.0},\n",
       " 'apprécie': {'V': 1.0},\n",
       " 'énergie': {'NC': 1.0},\n",
       " 'dépensée': {'VPP': 1.0},\n",
       " 'telle': {'ADJ': 1.0},\n",
       " 'pense': {'V': 1.0},\n",
       " 'espace': {'NC': 1.0},\n",
       " 'couleurs': {'NC': 1.0},\n",
       " 'silence': {'NC': 1.0},\n",
       " 'tel': {'ADJ': 1.0},\n",
       " 'ne': {'ADV': 1.0},\n",
       " 'peuvent': {'V': 1.0},\n",
       " 'élément': {'NC': 1.0},\n",
       " 'harmonie': {'NC': 1.0},\n",
       " 'espère': {'V': 1.0},\n",
       " \"d'_autres\": {'DET': 0.9259259259259259, 'PRO': 0.07407407407407407},\n",
       " 'projets': {'NC': 1.0},\n",
       " 'seront': {'V': 1.0},\n",
       " 'réalisés': {'VPP': 1.0},\n",
       " 'esprit': {'NC': 1.0},\n",
       " 'Les': {'DET': 1.0},\n",
       " 'enfants': {'NC': 1.0},\n",
       " 'pêcheurs': {'NC': 1.0},\n",
       " 'Ce': {'CLS': 0.28, 'DET': 0.68, 'PRO': 0.04},\n",
       " 'sont': {'V': 1.0},\n",
       " 'finalement': {'ADV': 1.0},\n",
       " 'CM2': {'NC': 1.0},\n",
       " 'Jacky': {'NPP': 1.0},\n",
       " 'Hedin': {'NPP': 1.0},\n",
       " 'venus': {'VPP': 0.8333333333333334, 'NC': 0.16666666666666666},\n",
       " 'mettre': {'VINF': 1.0},\n",
       " 'main': {'NC': 1.0},\n",
       " 'sac': {'NC': 1.0},\n",
       " 'poubelle': {'NC': 1.0},\n",
       " 'bonne': {'ADJ': 1.0},\n",
       " 'humeur': {'NC': 1.0},\n",
       " \"comme_s'\": {'CS': 1.0},\n",
       " 'ils': {'CLS': 1.0},\n",
       " 'étaient': {'V': 1.0},\n",
       " 'là': {'ADV': 1.0},\n",
       " 'participer': {'VINF': 1.0},\n",
       " 'jeu': {'NC': 1.0},\n",
       " 'plein-air': {'NC': 1.0},\n",
       " 'laissant': {'VPR': 1.0},\n",
       " 'soin': {'NC': 1.0},\n",
       " 'leurs': {'DET': 1.0},\n",
       " 'aînés': {'NC': 1.0},\n",
       " 'tirer': {'VINF': 1.0},\n",
       " 'morale': {'NC': 1.0},\n",
       " 'seconde': {'ADJ': 1.0},\n",
       " 'opération': {'NC': 1.0},\n",
       " 'déroulait': {'V': 1.0},\n",
       " 'parallèle': {'NC': 1.0},\n",
       " 'territoire': {'NC': 1.0},\n",
       " 'groupe': {'NC': 1.0},\n",
       " 'dizaine': {'NC': 1.0},\n",
       " 'chevaliers': {'NC': 1.0},\n",
       " 'gaule': {'NPP': 1.0},\n",
       " 'Barisienne': {'NPP': 1.0},\n",
       " 'ligne': {'NC': 1.0},\n",
       " 'nettoyer': {'VINF': 1.0},\n",
       " 'rives': {'NC': 1.0},\n",
       " 'curer': {'VINF': 1.0},\n",
       " 'ruisseau': {'NC': 1.0},\n",
       " 'serpente': {'V': 1.0},\n",
       " 'Sources': {'NPP': 0.3333333333333333, 'NC': 0.6666666666666666},\n",
       " 'avant_de': {'P': 1.0},\n",
       " 'jeter': {'VINF': 1.0},\n",
       " 'Ornain': {'NPP': 1.0},\n",
       " 'destinée': {'VPP': 1.0},\n",
       " 'faciliter': {'VINF': 1.0},\n",
       " 'remontée': {'NC': 1.0},\n",
       " 'fraye': {'NC': 1.0},\n",
       " 'truites': {'NC': 1.0},\n",
       " 'vers': {'P': 1.0},\n",
       " 'rivière': {'NC': 1.0},\n",
       " 'journée': {'NC': 1.0},\n",
       " 'nettoyage': {'NC': 1.0},\n",
       " 'environnement': {'NC': 1.0},\n",
       " 'placée': {'VPP': 1.0},\n",
       " 'bannière': {'NC': 1.0},\n",
       " 'Printemps': {'NC': 1.0},\n",
       " 'pas': {'ADV': 0.9868421052631579, 'NC': 0.013157894736842105},\n",
       " 'connu': {'VPP': 1.0},\n",
       " 'Fains-Véel': {'NPP': 1.0},\n",
       " 'succès': {'NC': 1.0},\n",
       " 'était': {'V': 1.0},\n",
       " 'droit': {'NC': 0.8571428571428571, 'ADJ': 0.14285714285714285},\n",
       " 'attendre': {'VINF': 1.0},\n",
       " 'Peut-être': {'ADV': 1.0},\n",
       " '-t-elle': {'CLS': 1.0},\n",
       " 'eu': {'VPP': 1.0},\n",
       " 'tort': {'NC': 1.0},\n",
       " 'veille': {'NC': 1.0},\n",
       " 'installer': {'VINF': 1.0},\n",
       " 'poubelles': {'NC': 1.0},\n",
       " 'fixes': {'ADJ': 1.0},\n",
       " 'synonymes': {'ADJ': 1.0},\n",
       " 'à_priori': {'ADV': 1.0},\n",
       " 'propreté': {'NC': 1.0},\n",
       " 'probablement': {'ADV': 1.0},\n",
       " 'effet': {'NC': 1.0},\n",
       " 'inciter': {'VINF': 1.0},\n",
       " 'gens': {'NC': 1.0},\n",
       " 'rester': {'VINF': 1.0},\n",
       " 'domicile': {'NC': 1.0},\n",
       " 'fêtent': {'V': 1.0},\n",
       " 'saint': {'NC': 1.0},\n",
       " 'Honoré': {'NPP': 1.0},\n",
       " 'mitrons': {'NC': 1.0},\n",
       " 'séduits': {'VPP': 1.0},\n",
       " 'odeur': {'NC': 1.0},\n",
       " 'pain': {'NC': 1.0},\n",
       " 'Depuis': {'P': 0.5, 'ADV': 0.5},\n",
       " 'quatre': {'DET': 0.4, 'ADJ': 0.6},\n",
       " 'ans': {'NC': 1.0},\n",
       " 'boulangers': {'NC': 1.0},\n",
       " 'français': {'ADJ': 0.8888888888888888, 'NC': 0.1111111111111111},\n",
       " 'clients': {'NC': 1.0},\n",
       " 'Saint-Honoré': {'NPP': 1.0},\n",
       " 'Ceux': {'PRO': 1.0},\n",
       " 'participé': {'VPP': 1.0},\n",
       " 'événement': {'NC': 1.0},\n",
       " 'faisant': {'VPR': 1.0},\n",
       " 'leur': {'DET': 0.8703703703703703, 'CLO': 0.12962962962962962},\n",
       " 'savoir-faire': {'NC': 1.0},\n",
       " 'recettes': {'NC': 1.0},\n",
       " 'CE1': {'NC': 1.0},\n",
       " 'Notre-Dame': {'NPP': 1.0},\n",
       " 'pris': {'VPP': 1.0},\n",
       " 'part': {'NC': 1.0},\n",
       " 'festivités': {'NC': 1.0},\n",
       " 'Ils': {'CLS': 1.0},\n",
       " 'reçus': {'VPP': 1.0},\n",
       " 'boulangerie': {'NC': 1.0},\n",
       " 'Leroy': {'NPP': 1.0},\n",
       " 'visiter': {'VINF': 1.0},\n",
       " 'fournil': {'NC': 1.0},\n",
       " 'surtout': {'ADV': 1.0},\n",
       " 'pétrir': {'VINF': 1.0},\n",
       " 'pâte': {'NC': 1.0},\n",
       " 'afin_de': {'P': 1.0},\n",
       " 'confectionner': {'VINF': 1.0},\n",
       " 'délicieux': {'ADJ': 1.0},\n",
       " 'pains': {'NC': 1.0},\n",
       " 'chocolat': {'NC': 1.0},\n",
       " 'dégustés': {'VPP': 1.0},\n",
       " 'heure': {'NC': 1.0},\n",
       " 'goûter': {'NC': 1.0},\n",
       " 'verre': {'NC': 1.0},\n",
       " 'jus': {'NC': 1.0},\n",
       " 'fruit': {'NC': 1.0},\n",
       " 'Quel': {'DET': 0.2857142857142857, 'ADJWH': 0.7142857142857143},\n",
       " 'bonheur': {'NC': 1.0},\n",
       " 'entendre': {'VINF': 1.0},\n",
       " 'boulangère': {'NC': 1.0},\n",
       " 'conter': {'VINF': 1.0},\n",
       " 'jeune': {'ADJ': 1.0},\n",
       " 'homme': {'NC': 1.0},\n",
       " 'dissipé': {'ADJ': 1.0},\n",
       " 'annonçant': {'VPR': 1.0},\n",
       " 'sa': {'DET': 1.0},\n",
       " 'nourrice': {'NC': 1.0},\n",
       " 'voulait': {'V': 1.0},\n",
       " 'prêtre': {'NC': 1.0},\n",
       " '!': {'PONCT': 1.0},\n",
       " 'entrer': {'VINF': 1.0},\n",
       " 'antre': {'NC': 1.0},\n",
       " 'gourmandise': {'NC': 1.0},\n",
       " 'secrets': {'NC': 1.0},\n",
       " 'magicien': {'NC': 1.0},\n",
       " 'Bref': {'ADV': 1.0},\n",
       " 'quittant': {'VPR': 1.0},\n",
       " 'chacun': {'PRO': 1.0},\n",
       " 'rêvait': {'V': 1.0},\n",
       " 'remettre': {'VINF': 1.0},\n",
       " 'Sylvie': {'NPP': 1.0},\n",
       " 'Daniel': {'NPP': 1.0},\n",
       " 'agenda': {'NC': 1.0},\n",
       " 'sportif': {'ADJ': 1.0},\n",
       " 'Football': {'NC': 1.0},\n",
       " 'seule': {'ADJ': 1.0},\n",
       " 'rencontre': {'NC': 0.8571428571428571, 'V': 0.14285714285714285},\n",
       " 'licenciés': {'NC': 1.0},\n",
       " 'FC': {'NPP': 1.0},\n",
       " 'équipe': {'NC': 1.0},\n",
       " 'B': {'NC': 1.0},\n",
       " 'chez': {'P': 1.0},\n",
       " 'seniors': {'NC': 1.0},\n",
       " 'évoluera': {'V': 1.0},\n",
       " 'à_partir_de': {'P': 1.0},\n",
       " '15': {'DET': 0.65, 'ADJ': 0.3, 'NC': 0.05},\n",
       " 'pelouse': {'NC': 1.0},\n",
       " 'terrain': {'NC': 1.0},\n",
       " 'honneur': {'NC': 1.0},\n",
       " 'face_à': {'P': 1.0},\n",
       " 'formation': {'NC': 1.0},\n",
       " 'Laneuville': {'NPP': 1.0},\n",
       " 'championnat': {'NC': 1.0},\n",
       " 'promotion': {'NC': 1.0},\n",
       " 'division': {'NC': 1.0},\n",
       " 'départementale': {'ADJ': 0.5, 'NC': 0.5},\n",
       " 'Cyclisme': {'NC': 1.0},\n",
       " 'cyclistes': {'NC': 1.0},\n",
       " 'vététistes': {'NC': 1.0},\n",
       " 'réunir': {'VINF': 1.0},\n",
       " 'matin': {'NC': 1.0},\n",
       " '9': {'DET': 0.5454545454545454, 'ADJ': 0.45454545454545453},\n",
       " 'place': {'NC': 0.9285714285714286, 'V': 0.07142857142857142},\n",
       " 'Jacques-Bailleurs': {'NPP': 1.0},\n",
       " \"à_l'_occasion_d'\": {'P': 1.0},\n",
       " 'sortie': {'NC': 1.0},\n",
       " 'entraînement': {'NC': 1.0},\n",
       " 'Cet': {'DET': 1.0},\n",
       " 'sera': {'V': 1.0},\n",
       " 'renouvelé': {'VPP': 1.0},\n",
       " 'demain': {'ADV': 1.0},\n",
       " 'mêmes': {'ADJ': 1.0},\n",
       " 'horaires': {'NC': 1.0},\n",
       " 'Billard': {'NC': 1.0},\n",
       " 'club': {'NC': 1.0},\n",
       " 'auront': {'V': 1.0},\n",
       " 'occasion': {'NC': 1.0},\n",
       " 'entraîner': {'VINF': 1.0},\n",
       " 'après-midi': {'NC': 1.0},\n",
       " 'salle': {'NC': 1.0},\n",
       " 'Jean-Mathieu': {'NPP': 1.0},\n",
       " 'Tir': {'NC': 1.0},\n",
       " 'tireurs': {'NC': 1.0},\n",
       " 'Vaux-Racine': {'NPP': 1.0},\n",
       " 'rendez-vous': {'NC': 1.0},\n",
       " 'tir': {'NC': 1.0},\n",
       " 'séance': {'NC': 1.0},\n",
       " 'Aviron': {'NC': 1.0},\n",
       " 'nautique': {'ADJ': 1.0},\n",
       " 'organisée': {'VPP': 1.0},\n",
       " 'rameurs': {'NC': 1.0},\n",
       " 'Rendez-vous': {'NC': 1.0},\n",
       " 'local': {'NC': 0.16666666666666666, 'ADJ': 0.8333333333333334},\n",
       " 'Décès': {'NC': 1.0},\n",
       " 'Guy': {'NPP': 1.0},\n",
       " 'Hosneld': {'NPP': 1.0},\n",
       " 'avait': {'V': 1.0},\n",
       " '44': {'DET': 1.0},\n",
       " 'Nous': {'CLS': 1.0},\n",
       " 'apprenons': {'V': 1.0},\n",
       " 'beaucoup': {'ADV': 1.0},\n",
       " 'tristesse': {'NC': 1.0},\n",
       " 'décès': {'NC': 1.0},\n",
       " 'suite_à': {'P': 1.0},\n",
       " 'douloureuse': {'ADJ': 1.0},\n",
       " 'maladie': {'NC': 1.0},\n",
       " 'âge': {'NC': 1.0},\n",
       " 'Né': {'VPP': 1.0},\n",
       " 'Abainville': {'NPP': 1.0},\n",
       " '11': {'ADJ': 0.9090909090909091, 'DET': 0.09090909090909091},\n",
       " 'février': {'NC': 1.0},\n",
       " '1955': {'NC': 1.0},\n",
       " 'arrivé': {'VPP': 1.0},\n",
       " 'tout': {'ADV': 0.2777777777777778,\n",
       "  'ADJ': 0.3333333333333333,\n",
       "  'PRO': 0.16666666666666666,\n",
       "  'DET': 0.2222222222222222},\n",
       " 'famille': {'NC': 1.0},\n",
       " 'Void-Vacon': {'NPP': 1.0},\n",
       " 'épousé': {'VPP': 1.0},\n",
       " 'Denise': {'NPP': 1.0},\n",
       " 'Pierrejean': {'NPP': 1.0},\n",
       " '26': {'ADJ': 1.0},\n",
       " 'octobre': {'NC': 1.0},\n",
       " '1974': {'NC': 1.0},\n",
       " 'union': {'NC': 1.0},\n",
       " 'nés': {'VPP': 1.0},\n",
       " 'fille': {'NC': 1.0},\n",
       " 'garçons': {'NC': 1.0},\n",
       " 'âgés': {'ADJ': 1.0},\n",
       " 'respectivement': {'ADV': 1.0},\n",
       " '24': {'DET': 0.7647058823529411, 'ADJ': 0.23529411764705882},\n",
       " '23': {'DET': 0.4, 'ADJ': 0.4, 'PRO': 0.2},\n",
       " 'consacré': {'VPP': 1.0},\n",
       " 'professionnelle': {'ADJ': 1.0},\n",
       " 'laiterie': {'NC': 1.0},\n",
       " 'Besnier': {'NPP': 1.0},\n",
       " 'Sorcy-Saint-Martin': {'NPP': 1.0},\n",
       " 'poste': {'NC': 1.0},\n",
       " 'chef': {'NC': 1.0},\n",
       " 'magasinier': {'NC': 1.0},\n",
       " 'comptait': {'V': 1.0},\n",
       " 'amis': {'NC': 1.0},\n",
       " 'personne': {'NC': 0.6666666666666666, 'PRO': 0.3333333333333333},\n",
       " 'très': {'ADV': 1.0},\n",
       " 'connue': {'VPP': 1.0},\n",
       " 'alentours': {'NC': 1.0},\n",
       " 'grand': {'ADJ': 1.0},\n",
       " 'dévouement': {'NC': 1.0},\n",
       " 'participation': {'NC': 1.0},\n",
       " 'associative': {'ADJ': 1.0},\n",
       " 'village': {'NC': 1.0},\n",
       " 'assuré': {'VPP': 1.0},\n",
       " 'présidence': {'NC': 1.0},\n",
       " 'société': {'NC': 0.8888888888888888, 'NPP': 0.1111111111111111},\n",
       " 'pêche': {'NC': 1.0},\n",
       " 'Gaule': {'NPP': 1.0},\n",
       " 'vidusienne': {'ADJ': 1.0},\n",
       " 'durant': {'P': 1.0},\n",
       " '12': {'DET': 0.8333333333333334,\n",
       "  'NC': 0.08333333333333333,\n",
       "  'ADJ': 0.08333333333333333},\n",
       " 'entré': {'VPP': 1.0},\n",
       " 'LAS': {'NPP': 1.0},\n",
       " 'handball': {'NC': 1.0},\n",
       " 'pratiqua': {'V': 1.0},\n",
       " 'près_de': {'P': 1.0},\n",
       " 'vingt-cinq': {'DET': 1.0},\n",
       " 'Sapeur-pompier': {'NC': 1.0},\n",
       " 'volontaire': {'ADJ': 1.0},\n",
       " 'responsabilité': {'NC': 1.0},\n",
       " 'premiers': {'ADJ': 1.0},\n",
       " 'cadets': {'NC': 1.0},\n",
       " 'sapeurs-pompiers': {'NC': 1.0},\n",
       " 'Meuse': {'NPP': 1.0},\n",
       " 'affectionnait': {'V': 1.0},\n",
       " 'jardinage': {'NC': 1.0},\n",
       " 'loisir': {'NC': 1.0},\n",
       " 'partagé': {'VPP': 1.0},\n",
       " 'épouse': {'NC': 0.75, 'V': 0.25},\n",
       " 'douleur': {'NC': 1.0},\n",
       " 'perdre': {'VINF': 1.0},\n",
       " 'il_y_a': {'P': 0.6153846153846154, 'V': 0.38461538461538464},\n",
       " 'quinze': {'DET': 1.0},\n",
       " 'laisse': {'V': 1.0},\n",
       " 'souvenir': {'NC': 0.6666666666666666, 'VINF': 0.3333333333333333},\n",
       " 'dévoué': {'ADJ': 1.0},\n",
       " 'généreux': {'ADJ': 1.0},\n",
       " 'jovial': {'ADJ': 1.0},\n",
       " 'toujours': {'ADV': 1.0},\n",
       " 'prêt': {'ADJ': 0.6666666666666666, 'NC': 0.3333333333333333},\n",
       " 'rendre': {'VINF': 1.0},\n",
       " 'service': {'NC': 1.0},\n",
       " 'obsèques': {'NC': 1.0},\n",
       " 'célébrées': {'VPP': 1.0},\n",
       " 'église': {'NC': 1.0},\n",
       " 'Nos': {'DET': 1.0},\n",
       " 'condoléances': {'NC': 1.0},\n",
       " 'Simon': {'NPP': 1.0},\n",
       " 'rien': {'PRO': 1.0},\n",
       " 'lui': {'CLO': 0.47368421052631576, 'PRO': 0.5263157894736842},\n",
       " 'échappe': {'V': 1.0},\n",
       " 'Photos': {'NC': 1.0},\n",
       " 'Jacques': {'NPP': 1.0},\n",
       " 'FISSIER': {'NPP': 1.0},\n",
       " 'avicole': {'ADJ': 1.0},\n",
       " 'Belfort': {'NPP': 1.0},\n",
       " '1922': {'NC': 1.0},\n",
       " 'Cela': {'PRO': 1.0},\n",
       " 'plus': {'ADV': 0.9568345323741008, 'P': 0.04316546762589928},\n",
       " 'trois-quarts': {'NC': 1.0},\n",
       " 'digne': {'ADJ': 1.0},\n",
       " 'nom': {'NC': 1.0},\n",
       " 'encourage': {'V': 1.0},\n",
       " 'éleveurs': {'NC': 1.0},\n",
       " 'amateurs': {'ADJ': 0.6666666666666666, 'NC': 0.3333333333333333},\n",
       " 'On': {'CLS': 1.0},\n",
       " 'peut': {'V': 1.0},\n",
       " 'estimer': {'VINF': 1.0},\n",
       " 'façon': {'NC': 1.0},\n",
       " 'astronomique': {'ADJ': 1.0},\n",
       " 'nombre': {'NC': 1.0},\n",
       " 'poules': {'NC': 1.0},\n",
       " 'canards': {'NC': 1.0},\n",
       " 'lapins': {'NC': 1.0},\n",
       " 'logiquement': {'ADV': 1.0},\n",
       " 'casseroles': {'NC': 1.0},\n",
       " 'profité': {'VPP': 1.0},\n",
       " 'engouement': {'NC': 1.0},\n",
       " 'Pour': {'P': 1.0},\n",
       " 'précis': {'ADJ': 1.0},\n",
       " \"c'\": {'CLS': 1.0},\n",
       " 'autour_de': {'P': 1.0},\n",
       " '1950': {'NC': 1.0},\n",
       " 'aviculture': {'NC': 1.0},\n",
       " 'vraiment': {'ADV': 1.0},\n",
       " 'réelle': {'ADJ': 1.0},\n",
       " 'importance': {'NC': 1.0},\n",
       " 'Claude': {'NPP': 1.0},\n",
       " 'Voici': {'V': 1.0},\n",
       " 'totalement': {'ADV': 1.0},\n",
       " 'cause': {'NC': 1.0},\n",
       " 'toutes': {'ADJ': 0.8275862068965517,\n",
       "  'DET': 0.13793103448275862,\n",
       "  'PRO': 0.034482758620689655},\n",
       " 'ces': {'DET': 1.0},\n",
       " 'petites': {'ADJ': 1.0},\n",
       " 'bêtes': {'NC': 1.0},\n",
       " 'su': {'VPP': 1.0},\n",
       " 'rassembler': {'VINF': 1.0},\n",
       " 'passionnés': {'ADJ': 0.25, 'NC': 0.75},\n",
       " 'quand': {'CS': 1.0},\n",
       " 'certains': {'PRO': 0.25, 'DET': 0.75},\n",
       " \"d'_entre\": {'P': 1.0},\n",
       " 'eux': {'PRO': 1.0},\n",
       " 'poulailler': {'NC': 1.0},\n",
       " 'cas': {'NC': 1.0},\n",
       " 'brave': {'ADJ': 1.0},\n",
       " 'Joseph': {'NPP': 1.0},\n",
       " 'Bari': {'NPP': 1.0},\n",
       " 'présent': {'ADJ': 1.0},\n",
       " 'comité': {'NC': 1.0},\n",
       " 'vedette': {'NC': 1.0},\n",
       " 'colombophilie': {'NC': 1.0},\n",
       " \"jusqu'_en\": {'P': 1.0},\n",
       " '1962': {'NC': 1.0},\n",
       " 'époque': {'NC': 1.0},\n",
       " 'laquelle': {'PROREL': 1.0},\n",
       " 'obligé': {'VPP': 1.0},\n",
       " 'quitter': {'VINF': 1.0},\n",
       " 'pigeonnier': {'NC': 1.0},\n",
       " 'Châtenois-les-Forges': {'NPP': 1.0},\n",
       " 'aller': {'VINF': 1.0},\n",
       " 'habiter': {'VINF': 1.0},\n",
       " 'réduit': {'ADJ': 0.25, 'VPP': 0.5, 'V': 0.25},\n",
       " 'Grand-Charmont': {'NPP': 1.0},\n",
       " 'cher': {'ADJ': 1.0},\n",
       " 'Jo': {'NPP': 1.0},\n",
       " 'élevait': {'V': 1.0},\n",
       " \"jusqu'_à\": {'P': 1.0},\n",
       " '120': {'DET': 1.0},\n",
       " 'pigeons': {'NC': 1.0},\n",
       " 'voyageurs': {'ADJ': 1.0},\n",
       " 'spécialistes': {'NC': 1.0},\n",
       " 'oublié': {'VPP': 1.0},\n",
       " 'champions': {'NC': 1.0},\n",
       " 'de_loin': {'ADV': 1.0},\n",
       " 'meilleur': {'ADJ': 1.0},\n",
       " '500': {'DET': 1.0},\n",
       " 'kilomètres': {'NC': 1.0},\n",
       " \"lors_d'\": {'P': 1.0},\n",
       " 'retour': {'NC': 1.0},\n",
       " 'Evreux': {'NPP': 1.0},\n",
       " 'Bayeux': {'NPP': 1.0},\n",
       " 'souvenirs': {'NC': 1.0},\n",
       " 'Heureusement': {'ADV': 1.0},\n",
       " 'adore': {'V': 1.0},\n",
       " 'consacrer': {'VINF': 1.0},\n",
       " 'temps': {'NC': 1.0},\n",
       " 'belle': {'ADJ': 1.0},\n",
       " 'résonne': {'V': 1.0},\n",
       " 'mille': {'DET': 1.0},\n",
       " 'chants': {'NC': 1.0},\n",
       " 'coqs': {'NC': 1.0},\n",
       " 'heureux': {'ADJ': 1.0},\n",
       " 'faut': {'V': 1.0},\n",
       " 'écouter': {'VINF': 1.0},\n",
       " 'voir': {'VINF': 1.0},\n",
       " 'ça': {'PRO': 1.0},\n",
       " 'Vivifiant': {'ADJ': 1.0},\n",
       " 'diable': {'NC': 1.0},\n",
       " 'Toi': {'PRO': 1.0},\n",
       " 'mon': {'DET': 1.0},\n",
       " 'toit': {'NC': 1.0},\n",
       " 'manoeuvre': {'NC': 1.0},\n",
       " 'maestria': {'NC': 1.0},\n",
       " 'vingt': {'DET': 0.5, 'ADJ': 0.5},\n",
       " 'minutes': {'NC': 1.0},\n",
       " 'aura': {'V': 1.0},\n",
       " 'fallu': {'VPP': 1.0},\n",
       " 'Thierry': {'NPP': 1.0},\n",
       " 'Guerry': {'NPP': 1.0},\n",
       " 'chauffeur-routier': {'NC': 1.0},\n",
       " 'Caillaud': {'NPP': 1.0},\n",
       " 'entreprise': {'NC': 1.0},\n",
       " 'charpente': {'NC': 1.0},\n",
       " 'chargée': {'VPP': 1.0},\n",
       " 'toiture': {'NC': 1.0},\n",
       " 'cours': {'NC': 1.0},\n",
       " 'tennis': {'NC': 1.0},\n",
       " 'couverts': {'ADJ': 1.0},\n",
       " 'pénétrer': {'VINF': 1.0},\n",
       " 'enceinte': {'NC': 0.125, 'ADJ': 0.875},\n",
       " 'complexe': {'NC': 0.5, 'ADJ': 0.5},\n",
       " 'semi-remorque': {'NC': 1.0},\n",
       " 'lequel': {'PROREL': 1.0},\n",
       " 'chargées': {'VPP': 1.0},\n",
       " 'quatorze': {'DET': 0.5, 'ADJ': 0.5},\n",
       " 'tonnes': {'NC': 1.0},\n",
       " 'éléments': {'NC': 1.0},\n",
       " 'bois': {'NC': 1.0},\n",
       " 'trente': {'DET': 1.0},\n",
       " 'mètres': {'NC': 1.0},\n",
       " 'long': {'ADJ': 1.0},\n",
       " 'destinés': {'VPP': 1.0},\n",
       " 'couverture': {'NC': 1.0},\n",
       " 'stade': {'NC': 1.0},\n",
       " 'centimètre': {'NC': 1.0},\n",
       " 'près': {'ADV': 1.0},\n",
       " 'cabine': {'NC': 1.0},\n",
       " 'véhicule': {'NC': 1.0},\n",
       " 'pu': {'VPP': 1.0},\n",
       " 'passer': {'VINF': 1.0},\n",
       " 'poteaux': {'NC': 1.0},\n",
       " 'portail': {'NC': 1.0},\n",
       " 'principal': {'ADJ': 1.0},\n",
       " 'chapeau': {'NC': 1.0},\n",
       " 'enlevé': {'VPP': 1.0},\n",
       " 'spectaculaire': {'ADJ': 1.0},\n",
       " 'dirigée': {'VPP': 1.0},\n",
       " 'pilote': {'NC': 1.0},\n",
       " 'convoi': {'NC': 1.0},\n",
       " 'exceptionnel': {'ADJ': 1.0},\n",
       " 'Parti': {'VPP': 0.5, 'NC': 0.5},\n",
       " 'Cholet': {'NPP': 1.0},\n",
       " '-LRB-': {'PONCT': 1.0},\n",
       " 'Maine': {'NPP': 1.0},\n",
       " 'Loire': {'NPP': 1.0},\n",
       " '-RRB-': {'PONCT': 1.0},\n",
       " 'chauffeur': {'NC': 1.0},\n",
       " 'demi': {'NC': 1.0},\n",
       " 'pointes': {'NC': 1.0},\n",
       " 'vitesse': {'NC': 1.0},\n",
       " '70': {'DET': 1.0},\n",
       " 'km': {'NC': 1.0},\n",
       " '/': {'P': 0.8222222222222222, 'PONCT': 0.1, 'CC': 0.07777777777777778},\n",
       " 'moins': {'P': 0.037037037037037035, 'ADV': 0.9629629629629629},\n",
       " 'nuit': {'NC': 0.875, 'V': 0.125},\n",
       " 'repos': {'NC': 1.0},\n",
       " 'couvrir': {'VINF': 1.0},\n",
       " 'distance': {'NC': 1.0},\n",
       " '600': {'DET': 1.0},\n",
       " 'malaise': {'NC': 1.0},\n",
       " 'semble': {'V': 1.0},\n",
       " 'conducteur': {'NC': 1.0},\n",
       " 'pourrait': {'V': 1.0},\n",
       " 'origine': {'NC': 1.0},\n",
       " 'épileptique': {'ADJ': 1.0},\n",
       " 'soit': {'VS': 0.6341463414634146, 'CC': 0.36585365853658536},\n",
       " 'accident': {'NC': 1.0},\n",
       " 'peu': {'ADV': 0.9565217391304348, 'PRO': 0.043478260869565216},\n",
       " 'classique': {'ADJ': 1.0},\n",
       " \"S'\": {'CLR': 0.6, 'CS': 0.4},\n",
       " 'apercevant': {'VPR': 1.0},\n",
       " 'mari': {'NC': 1.0},\n",
       " 'soudainement': {'ADV': 1.0},\n",
       " 'perdu': {'VPP': 1.0},\n",
       " 'connaissance': {'NC': 1.0},\n",
       " 'passagère': {'NC': 1.0},\n",
       " 'avant': {'ADV': 0.02564102564102564,\n",
       "  'P': 0.8974358974358975,\n",
       "  'NC': 0.05128205128205128,\n",
       "  'ADJ': 0.02564102564102564},\n",
       " 'tenté': {'VPP': 1.0},\n",
       " 'garder': {'VINF': 1.0},\n",
       " 'droite': {'ADJ': 0.3333333333333333, 'NC': 0.6666666666666666},\n",
       " 'saisissant': {'VPR': 1.0},\n",
       " 'volant': {'NC': 1.0},\n",
       " 'Malgré': {'P': 1.0},\n",
       " 'parcourue': {'VPP': 1.0},\n",
       " 'rapide': {'ADJ': 1.0},\n",
       " 'couple': {'NC': 1.0},\n",
       " 'originaire': {'ADJ': 1.0},\n",
       " 'Territoire': {'NC': 1.0},\n",
       " 'Hier': {'ADV': 1.0},\n",
       " 'début': {'NC': 1.0},\n",
       " 'soirée': {'NC': 1.0},\n",
       " 'route': {'NC': 1.0},\n",
       " 'reliant': {'VPR': 1.0},\n",
       " 'Danjoutin': {'NPP': 1.0},\n",
       " 'en_partie': {'ADV': 1.0},\n",
       " 'neutralisée': {'VPP': 1.0},\n",
       " 'dépanneur': {'NC': 1.0},\n",
       " ...}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.lexic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning corpus grammar\n",
      "Put in chomsky normal form\n",
      "building OOV from corpus\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "#Grammar\n",
    "print(\"learning corpus grammar\")\n",
    "train=CorpusManager(corpus_train)\n",
    "train.erase_dash()\n",
    "train.build_pcfg()\n",
    "train.build_lexic()\n",
    "train.find_term_node()\n",
    "\n",
    "print(\"Put in chomsky normal form\")\n",
    "train.non_sol_term()\n",
    "train.binarize()\n",
    "train.unary_rule()\n",
    "train.erase_last_unary()\n",
    "train.build_mat_grammar()\n",
    "\n",
    "train.build_nodes()\n",
    "train.build_sentences()\n",
    "train.build_bigram()\n",
    "train.build_unigram()\n",
    "\n",
    "#OOV\n",
    "print(\"building OOV from corpus\")\n",
    "oov=OOV(lexic_train=train.lexic,path_embeddings='polyglot-fr.pkl',unigram=train.unigram,bigram=train.bigram)\n",
    "\n",
    "print(\"finish\")\n",
    "val=CorpusManager(corpus_dev)\n",
    "test=CorpusManager(corpus_test)\n",
    "val.erase_dash()\n",
    "test.erase_dash()\n",
    "val.build_sentences()\n",
    "test.build_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result From a text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25% (1 of 4) |######                    | Elapsed Time: 0:00:00 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (4 of 4) |##########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYK\n",
      "( (SENT (DET Le)(NC Procès)(PP (P en)(NP (ADJ première)(NC instance)))))\n",
      "2 / 174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25% (4 of 16) |######                   | Elapsed Time: 0:00:02 ETA:   0:00:09"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ea9120c7b051>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mcorrected_sent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_a_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_sent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPCYK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrected_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0msol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_bracket_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_sent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrected_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdic_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SENT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#add start sent here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-413e7d9da97a>\u001b[0m in \u001b[0;36mPCYK\u001b[0;34m(self, corrected_sentence)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                                 \u001b[0mprob_prod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                                 \u001b[0mprob_binary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob_prod\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_par\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mprob_binary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "corpus_to_file=[\" \".join(x) for x in test.sentences if len(x)<=20]\n",
    "target_bracket_sent=[x for x,y in zip(test.corpus,test.sentences) if len(y)<=20]\n",
    "\n",
    "\n",
    "input_path = 'input.txt'\n",
    "with open('input.txt','w') as f:\n",
    "       for line in corpus_to_file:\n",
    "            f.write(line)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "with open('input.txt','r') as f:\n",
    "    lines = [line.strip('\\n') for line in f]\n",
    "\n",
    "sentence_to_parse=[]\n",
    "for line in lines:\n",
    "    sentence_to_parse.append(line.split())\n",
    "\n",
    "sentence_to_parse[0]\n",
    "target_bracket_sent[0]\n",
    "\n",
    "\n",
    "pred_sent=[]\n",
    "for i,sent in enumerate(sentence_to_parse):\n",
    "        print(i+1,\"/\",len(sentence_to_parse))\n",
    "        start_sent=sentence_to_parse[i]\n",
    " \n",
    "        corrected_sent=oov.process_a_sentence(start_sent,2,2,0.2)\n",
    "        back, prob=train.PCYK(corrected_sent)\n",
    "        sol=train.build_bracket_sent(start_sent,back,len(corrected_sent)-1,0,train.dic_nodes[\"SENT\"]) #add start sent here\n",
    "        \n",
    "        print(\"CYK\")\n",
    "        res_cyk=\"( (SENT \"+sol+\"))\"\n",
    "        pred_sent.append(train.unchomsky_bracket_sentence(res_cyk))\n",
    "        print(train.unchomsky_bracket_sentence(res_cyk))\n",
    "\n",
    "#np.save(\"pred_inf20.npy\",pred_small_sent)\n",
    "#np.save(\"target_inf20.npy\",target_bracket_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 10) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40% (4 of 10) |##########               | Elapsed Time: 0:00:01 ETA:   0:00:02"
     ]
    }
   ],
   "source": [
    "np.random.seed(2367)\n",
    "l=list(np.arange(90))\n",
    "np.random.shuffle(l)\n",
    "#Only small one to test..\n",
    "sentence_to_parse=[x for x in val.sentences if len(x)<=20]\n",
    "target_bracket_sent=[x for x,y in zip(val.corpus,val.sentences) if len(y)<=20]\n",
    "\n",
    "sentence_to_parse=[sentence_to_parse[i] for i in l]\n",
    "target_bracket_sent=[target_bracket_sent[i] for i in l]\n",
    "\n",
    "for hyper_lambd in [0.05,0.2,0.5,0.8,0.95]:\n",
    "\n",
    "    pred_small_val_sent=[]\n",
    "    for i,sent in enumerate(sentence_to_parse):\n",
    "            print(i+1,\"/\",len(sentence_to_parse))\n",
    "            start_sent_bracket=target_bracket_sent[i]\n",
    "            start_sent=sentence_to_parse[i]\n",
    "\n",
    "            corrected_sent=oov.process_a_sentence(start_sent,20,20,hyper_lambd)\n",
    "            back, prob=train.PCYK(corrected_sent)\n",
    "            sol=train.build_bracket_sent(start_sent,back,len(corrected_sent)-1,0,train.dic_nodes[\"SENT\"]) #add start sent here\n",
    "\n",
    "            res_cyk=\"( (SENT \"+sol+\"))\"\n",
    "            pred_small_val_sent.append(train.unchomsky_bracket_sentence(res_cyk))\n",
    "\n",
    "\n",
    "\n",
    "    np.save(\"pred_small_20_20_\"+str(hyper_lambd)+\".npy\",pred_small_val_sent)\n",
    "    np.save(\"target_small_val.npy\",target_bracket_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring/ Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_pred=\"pred_small_4_4_\"+str(hyper_lambd)+\".npy\"\n",
    "len(np.load(path_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Unmatched !\n",
      "gold sentence:18\n",
      "test sentence:1\n",
      "------------------------------\n",
      "Length Unmatched !\n",
      "gold sentence:18\n",
      "test sentence:1\n",
      "------------------------------\n",
      "Length Unmatched !\n",
      "gold sentence:18\n",
      "test sentence:1\n",
      "------------------------------\n",
      "Length Unmatched !\n",
      "gold sentence:18\n",
      "test sentence:1\n",
      "------------------------------\n",
      "Length Unmatched !\n",
      "gold sentence:18\n",
      "test sentence:1\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#ok pad the bracket sentence and the tardet sentence \n",
    "#write the result on a text file without the beginning of the bracket and eval with gold etc..\n",
    "for hyper_lambd in [0.05,0.2,0.5,0.8,0.95]:\n",
    "    path_pred=\"pred_small_4_4_\"+str(hyper_lambd)+\".npy\"\n",
    "    path_target=\"target_small_val.npy\"\n",
    "\n",
    "    target_bracket=np.load(path_target)\n",
    "    pred_bracket=np.load(path_pred)\n",
    "\n",
    "    target_bracket=[x[2:][:-1] for x in target_bracket]\n",
    "    pred_bracket=[x[2:][:-1] for x in pred_bracket]\n",
    "\n",
    "    gold_path='gold_corpus.txt'\n",
    "    test_path='test_corpus.txt'\n",
    "\n",
    "    with open(gold_path,'w') as f:\n",
    "           for line in target_bracket:\n",
    "                f.write(line)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "    with open(test_path,'w') as f:\n",
    "           for line in pred_bracket:\n",
    "                f.write(line)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "    result_path = \"res_\"+\"pred_small_20_20_\"+str(hyper_lambd)+\".txt\"\n",
    "\n",
    "    score=scorer.Scorer()\n",
    "    score.evalb(gold_path, test_path, result_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results final on the test part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_to_parse=test.sentences\n",
    "target_bracket_sent=test.corpus\n",
    "\n",
    "pred_all_sent=[]\n",
    "for i,sent in enumerate(sentence_to_parse):\n",
    "        print(i+1,\"/\",len(sentence_to_parse))\n",
    "        start_sent_bracket=target_bracket_sent[i]\n",
    "        start_sent=sentence_to_parse[i]\n",
    "\n",
    "       \n",
    "        corrected_sent=oov.process_a_sentence(start_sent,2,2,0.2)\n",
    "        back, prob=train.PCYK(corrected_sent)\n",
    "        sol=train.build_tree(start_sent,back,len(corrected_sent)-1,0,train.dic_nodes[\"SENT\"]) #add start sent here\n",
    "  \n",
    "        res_cyk=\"( (SENT \"+sol+\"))\"\n",
    "        pred_all_sent.append(train.unchomsky(res_cyk))\n",
    "        \n",
    "\n",
    "np.save(\"pred_all.npy\",pred_all_sent)\n",
    "np.save(\"target_all.npy\",target_bracket_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test CYK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCYK2(self,corrected_sentence):\n",
    "    \n",
    "        n_nodes=len(list(self.dic_nodes.keys()))\n",
    "        n=len(corrected_sentence)\n",
    "        prob = np.zeros((n,n,n_nodes))\n",
    "        back = np.zeros((n,n,n_nodes),dtype=np.ndarray)\n",
    "\n",
    "\n",
    "        for s,word in enumerate(corrected_sentence):\n",
    "            for pos in self.lexic[word]:\n",
    "                v = self.dic_nodes[pos]\n",
    "                prob[0,s,v] = self.lexic[word][pos]\n",
    "    \n",
    "    #browse unaries\n",
    "        for s in range(n):\n",
    "            for l_h_s, prod in self.grammar.items():\n",
    "                for kb,vb in prod.items():\n",
    "                    r_h_s=kb.split(\"_\")\n",
    "                    if len(r_h_s)==1:\n",
    "                        v1 = self.dic_nodes[l_h_s]\n",
    "                        v2 = self.dic_nodes[r_h_s[0]]\n",
    "                        prob_splitting = prob[0,s,v2] * vb #proba du lexic *proba que ça arrive\n",
    "                        if prob[0,s,v2]>0 and prob_splitting > prob[0,s,v1]: \n",
    "                            prob[0,s,v1] = prob_splitting\n",
    "                            back[0,s,v1] = (0,v2,-1)\n",
    "\n",
    "        for l in progressbar.progressbar(range(2,n+1)): #length\n",
    "            for s in range(1,n-l+2): #start\n",
    "                for p in range(0,l): #cut cursor\n",
    "                    for l_h_s, r_h_s in self.grammar.items(): \n",
    "                        a = self.dic_nodes[l_h_s]\n",
    "                        \n",
    "                        for kp,vp in r_h_s.items():    \n",
    "                            sp=kp.split(\"_\") \n",
    "                            if len(sp)==2:\n",
    "                                \n",
    "                                b = self.dic_nodes[sp[0]]\n",
    "                                c = self.dic_nodes[sp[1]]\n",
    "                                \n",
    "                                prob_prod = vp\n",
    "                                prob_splitting = prob_prod * prob[p-1,s-1,b] * prob[l-p-1,s+p-1,c]\n",
    "                               \n",
    "                                if prob[p-1,s-1,b] > 0 and prob[l-p-1,s+p-1,c] > 0 and prob[l-1,s-1,a] < prob_splitting:\n",
    "                                    \n",
    "                                    #print(l_h_s,\"->\",sp[0],sp[1])\n",
    "                                    prob[l-1,s-1,a] = prob_splitting\n",
    "                                    back[l-1,s-1,a] = (p,b,c)\n",
    "            \n",
    "        return(back, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chomsky_tree={}\n",
    "\n",
    "dic1={\"S\":{ \"A_B\":1.0,\"h\":1.0}}\n",
    "dic2={\"A\":{\"C_C\":0.1, \"a\":0.7,\"c\":0.2}}\n",
    "dic3={\"B\":{\"B_C\":0.8,\"b\":0.2}}\n",
    "dic4={\"C\":{\"C_B\":0.7,\"B_A\":0.2,\"c\":0.1}}\n",
    "\n",
    "chomsky_tree.update(dic1)\n",
    "chomsky_tree.update(dic2)\n",
    "chomsky_tree.update(dic3)\n",
    "chomsky_tree.update(dic4)\n",
    "chomsky_tree\n",
    "\n",
    "lexic={\"banane\":{\"a\":0.8,\"c\":0.2} ,\"chocolat\":{\"b\":0.1, \"c\":0.1 ,\"h\":0.8},\"bleu\":{\"b\":1}}\n",
    "sent=[\"chocolat\",\"chocolat\",\"bleu\",\"chocolat\"]\n",
    "\n",
    "#min imal examples, shpuld work as it work in the other notebook\n",
    "ctest=CorpusManager(corpus_train)\n",
    "ctest.grammar=chomsky_tree\n",
    "ctest.lexic=lexic\n",
    "ctest.build_nodes()\n",
    "\n",
    "sent=[\"chocolat\",\"chocolat\",\"bleu\",\"chocolat\"]\n",
    "back, prob =ctest.PCYK2(sent) # pour le test CYK 2\n",
    "\n",
    "sol=ctest.build_tree(sent,sent,back,len(sent)-1,0,ctest.dic_nodes[\"S\"]) #add start sent here\n",
    "sol    \n",
    "#back, prob =PCYK(test,sent)\n",
    "##ok ça marche !!!\n",
    "##le CYK MARCHE SUR UN ARBRE BIEN CONSTRUIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def build_tree(self,start_sent,back,l,start,POS,S=''): \n",
    "       \n",
    "        indexes = back[l,start,POS]\n",
    "        if indexes == 0: \n",
    "            return start_sent[start]\n",
    "        else: \n",
    "            p,left,right = indexes\n",
    "            if  left!=0 and right == -1 : \n",
    "                return \"(\" + list(self.dic_nodes.keys())[left]+ \" \" + start_sent[start] + \")\"\n",
    "            #print(non_terminals[pos],non_terminals[b],non_terminals[c])\n",
    "            S += \"(\" +list(self.dic_nodes.keys())[left]+\" \"+ self.build_tree(start_sent,back,p-1,start,left) +\")\"\n",
    "            S += \"(\" +list(self.dic_nodes.keys())[right]+\" \"+ self.build_tree(start_sent,back,l-p,start+p,right) +\")\" \n",
    "\n",
    "        return S\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
