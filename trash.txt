"""

def decode(listw,lists):
	# = list(itertools.permutations([len(s) for s in lists]))
	sentences = [element for element in itertools.product(*listw)]
	scores = []
	for sentence in sentences:
		s = 0
		for i,w in enumerate(sentence):
			if i==0: 
				s+= lists[0][listw.index[w]]


def extract_POS(sentence, lexicon):
	POS = []
	for w in sentence:
		dic = lexicon[w]
		POS.append(max(dic, key=dic.get))

	return POS

def sum_dic(dic1,dic2):
	dic = {}
	keys = set(dic1.keys()).intersection(dic2.keys())
	for k in keys:
		dic[k] = dic1.get(k,0) + dic2.get(k,0)
	return dic


def countOOV(sentence, pcfg):
	c = 0
	for w in sentence:
		if w not in pcfg.lexicon_:
			print("Word {} not in vocabulary".format(word))
			c+=1
	return c



def get_POS_old(sentence, pcfg, w2v=None, ngram=None, n1=2, n2=2, l=0.2):

	listw = []
	lists = []

	for i,word in enumerate(sentence):

		if word in pcfg.lexicon_:

			#for j in range(len(words)):
			listw.append([word])

			if i==0:
				lists.append([proba_interpolation(word,l,ngram)])
			else:
				score = []
				for w0 in listw[i-1]:
					score.append(proba_interpolation(word,l,ngram,w0))
				lists.append(score)

		else:
			print("Word {} not in vocabulary".format(word))
			score = []
			word_leven = w2v.most_similar_levenshtein(word,k=n1)
			word_cosine = w2v.most_similar_embeddings(word, k=n2)
			candidates = word_leven + word_cosine
			listw.append(candidates)
			
			if i==0:
				for w in candidates:
					score.append(proba_interpolation(w,l,ngram))
				lists.append(score)

			else:
				for w1 in candidates:
					for w0 in listw[i-1]:
						score.append(proba_interpolation(w1,l,ngram,w0))
				lists.append(score)

	return decode(listw, lists)#decode(listw, lists)
"""


#n = len(sentence)
#c = countOOV(sentence, pcfg)
#scores = np.zeros((c*n1*n2,n))
#words = [[]]*(c*n1*n2)
#return [get_POS_w(w, pcfg, w2v, ngram,n1, n2) for w in sentence]
#print(word_leven, word_cosine)
#print(pcfg.lexicon_[word_leven])
#print(pcfg.lexicon_[word_cosine])
#dic = sum_dic(pcfg.lexicon_[word_leven],pcfg.lexicon_[word_cosine])
#return max(dic, key=dic.get)
#dic = pcfg.lexicon_[word]
#print(dic)
#return max(dic, key=dic.get)



	def get_best_sentence_bourrin(self, sentence, n1=2, n2=2, l=0.2):
		"""FILL = improve since all words do not communicate ? """

		listw = []

		# get nearest neighbours
		for i,word in enumerate(sentence):
			if word in vocabulary:
				listw.append([word])
			else:
				print("Word {} not in vocabulary".format(word))
				word_leven = w2v.most_similar_levenshtein(word,k=n1, damerau = self.damerau)
				word_cosine = w2v.most_similar_embeddings(word, k=n2)
				listw.append(word_leven + word_cosine)

		# all possible production of sentences
		sentences = [element for element in itertools.product(*listw)]

		best_sentence = []
		maxi_score = -np.infty

		for s in sentences:
			score = 0 
			for i,w in enumerate(s):
				if i==0: 
					score+= proba_interpolation(w,l,ngram)
				else: 
					score += proba_interpolation(w,l,ngram,s[i-1])

			if score>maxi_score:
				best_sentence = s
				maxi_score = score

		return best_sentence





